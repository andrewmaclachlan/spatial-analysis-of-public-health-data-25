[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial analysis of public health data",
    "section": "",
    "text": "Welcome\nHello, this website  holds the material used to support the Spatial Analysis of public health data, part of the MSc in Modelling for Global Health at the University of Oxford.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Spatial analysis of public health data",
    "section": "",
    "text": "The Bartlett Centre for Advanced Spatial Analysis, https://www.andymac.uk/↩︎\nhttps://cls.ucl.ac.uk/team/laura-sheppard/↩︎\nThe Bartlett Centre for Advanced Spatial Analysis, https://adamdennett.co.uk/↩︎\nCentre for Behaviour Change, https://www.chiaragericke.co.uk/about-me↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software installation",
    "section": "",
    "text": "QGIS\nQGIS is an open-source graphic user interface GIS with many community developed add on packages that (or plugins) that provide additional functionality to the software. In this module we will primarly use QGIS to check our spatial data.\nTo get QGIS on your personal machine go to: https://qgis.org/en/site/forusers/download.html\nI install the OSGeo4W version. The nature of open-source means that several programs will rely on each other for features. OSGeo4W tracks all the shared requirements and does not install any duplicates.\nWhen you click through the dialogue boxes you need to search for QGIS in the OSGeo4W setup and click the refresh button so it changes from skip to install….\nContinuous and discrete data. Source: GIS Stackexchange",
    "crumbs": [
      "Software installation"
    ]
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "Software installation",
    "section": "R",
    "text": "R\nR is both a programming language and software environment, originally designed for statistical computing and graphics. R’s great strength is that it is open-source, can be used on any computer operating system and free for anyone to use and contribute to. Because of this, it is rapidly becoming the statistical language of choice for many academics and has a huge user community with people constantly contributing new packages to carry out all manner of statistical, graphical and importantly for us, geographical tasks.\n\nBasics\nThe advised method for using R is to download it to your personal machine, so you can use it in future without any issues.\nTo use “R” we need to bits of software:\n\nR itself: https://cran.rstudio.com/\nThen also RStudio: https://www.rstudio.com/products/rstudio/download/#download\n\nR is the core software and RStudio gives a simpler interface for us to use it. If you used R (which you could) you’d be just typing code, with no other features. RStudio gives us other features to make this much easier, just like how you would never type a report in notepad (Windows) or notes (on Apple), you’d use the word processor to make it easier. Chester Ismay and Albert Y. Kim explain this with a car analogy:\n\n\n\n\n\nR vs RStudio. Source: A ModernDive into R and the Tidyverse, Ismay and Kim, 2021\n\n\n\n\nR and RStudio are software that require updating over time. It’s easy to forget!\n\n\nPackages\nBase R (controlled or driven through RStudio) is very limited. As a result people develop packages to make data loading, wrangling, analysis and visulisation much easier than having to write all the code. All packages will have lots of functions that just look up code from the package to make what you are doing easier (we will go into this in more detail later).\nPackages develop over time and have different versions (like software). This can cause some issues with updates and code not working like it did once before. This book means i have to run all the code anyway, so every year it is checked and most packages are updated.\nThere are many errors and issues that can arise with package installation. You should read the following section now so you are aware of it, but you won’t need it until you hit an issue.\nThe most common package errors and issues are:\n\nThe package relies on another package which you might not have — this is called a dependency. Think about building a house — moving in and living in the house is dependent on the foundations, the walls, the windows. The solution is to install the package you are missing.\nThe package has a new version — simply update it, in RStudio the package tab (which we will see in the module) has an update option.\nYour version of R might not support the package. This is unlikely as I update this every year. But to solve it either use an older version of the package and/or update your version of R.\nThere is an error with the package. If all else fails i remove the package and the re-install it. Again, in the package tab there is a small cross next to each package, clicking it let’s you remove it.\n\nIf the problem persists then assuming the error is package xxx is not available consult the relevant stack overflow question",
    "crumbs": [
      "Software installation"
    ]
  },
  {
    "objectID": "01_geographic_data.html",
    "href": "01_geographic_data.html",
    "title": "1  Geographic information",
    "section": "",
    "text": "1.1 Learning outcomes\nBy the end of this practical you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#learning-outcomes",
    "href": "01_geographic_data.html#learning-outcomes",
    "title": "1  Geographic information",
    "section": "",
    "text": "Describe and explain GIS data formats and databases\nSource and pre-process spatial data\nLoad and undertaken some basic manipulation of spatial data\nCreate some maps",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#the-basics-of-geographic-information",
    "href": "01_geographic_data.html#the-basics-of-geographic-information",
    "title": "1  Geographic information",
    "section": "1.2 The Basics of geographic information",
    "text": "1.2 The Basics of geographic information\nGeographic data, geospatial data or geographic information is data that identifies the location of features on Earth. There are two main types of data which are used in GIS applications to represent the real world. Vectors that are composed of points, lines and polygons and rasters that are grids of cells with individual values…\n\n\n\n\n\nTypes of spatial data. Source: Spatial data models\n\n\n\n\nIn the above example the features in the real world (e.g. lake, forest, marsh and grassland) have been represented by points, lines and polygons (vector) or discrete grid cells (raster) of a certain size (e.g. 1 x 1m) specifying land cover type.\n\n1.2.1 Data types in statistics\nBefore we go any further let’s just quick go over the different types of data you might encounter\nContinuous data can be measured on some sort of scale and can have any value on it such as height and weight.\nDiscrete data have finite values (meaning you can finish counting something). It is numeric and countable such as number of shoes or the number of computers within the classroom.\nFoot length would be continuous data but shoe size would be discrete data.\n\n\n\n\n\nContinuous and discrete data. Source: Allison Horst data science and stats illustrations\n\n\n\n\nNominal (also called categorical) data has labels without any quantitative value such as hair colour or type of animal. Think names or categories - there are no numbers here.\nOrdinal, similar to categorical but the data has an order or scale, for example if you have ever seen the chilli rating system on food labels or filled a happiness survey with a range between 1 and 10 — that’s ordinal. Here the order matters, but not the difference between them.\nBinary data is that that can have only two possible outcomes, yes and no or shark and not shark.\n\n\n\n\n\nNominal, ordinal and binary data. Source: Allison Horst data science and stats illustrations\n\n\n\n\n\n\n1.2.2 Important GIS data formats\nThere are a number of commonly used geographic data formats that store vector and raster data that you will come across during this course and it’s important to understand what they are, how they represent data and how you can use them.\n\n1.2.2.1 Shapefiles\nPerhaps the most commonly used GIS data format is the shapefile. Shapefiles were developed by ESRI, one of the first and now certainly the largest commercial GIS company in the world. Despite being developed by a commercial company, they are mostly an open format and can be used (read and written) by a host of GIS Software applications.\nA shapefile is actually a collection of files —- at least three of which are needed for the shapefile to be displayed by GIS software. They are:\n\n.shp - the file which contains the feature geometry\n.shx - an index file which stores the position of the feature IDs in the .shp file\n.dbf - the file that stores all of the attribute information associated with the coordinates – this might be the name of the shape or some other information associated with the feature\n.prj - the file which contains all of the coordinate system information (the location of the shape on Earth’s surface). Data can be displayed without a projection, but the .prj file allows software to display the data correctly where data with different projections might be being used\n\nOn Twitter and want to see the love for shapefiles….have a look at the shapefile account\n\n\n1.2.2.2 GeoJSON\nGeoJSON Geospatial Data Interchange format for JavaScript Object Notation is becoming an increasingly popular spatial data format, particularly for web-based mapping as it is based on JavaScript Object Notation. Unlike a shapefile in a GeoJSON, the attributes, boundaries and projection information are all contained in the same file.\n\n\n1.2.2.3 Raster data\nMost raster data is now provided in GeoTIFF (.tiff) format, which stands for Geostarionary Earth Orbit Tagged Image File. The GeoTIFF data format was created by NASA and is a standard public domain format. All necesary information to establish the location of the data on Earth’s surface is embedded into the image. This includes: map projection, coordinate system, ellipsoid and datum type.\n\n\n1.2.2.4 Geodatabase\nA geodatabase is a collection of geographic data held within a database. Geodatabases were developed by ESRI to overcome some of the limitations of shapefiles. They come in two main types: Personal (up to 1 TB) and File (limited to 250 - 500 MB), with Personal Geodatabases storing everything in a Microsoft Access database (.mdb) file and File Geodatabases offering more flexibility, storing everything as a series of folders in a file system. In the example below we can see that the FCC_Geodatabase (left hand pane) holds multiple points, lines, polygons, tables and raster layers in the contents tab.\n\n\n\n\n\n\n\n\n\n\n\n1.2.2.5 GeoPackage\n\n\n\n\n\nGeoPacakge logo\n\n\n\n\nA GeoPackage is an open, standards-based, platform-independent, portable, self-describing, compact format for transferring geospatial data. It stores spatial data layers (vector and raster) as a single file, and is based upon an SQLite database, a widely used relational database management system, permitting code based, reproducible and transparent workflows. As it stores data in a single file it is very easy to share, copy or move.\n\n\n1.2.2.6 SpatiaLite\n\n\n\n\n\nSpatialLite logo\n\n\n\n\nSpatialLite is an open-source library that extends SQLite core. Support is fairly limited and most software that supports SpatiaLite also supports GeoPackage, as they both build upon SQLite. It doesn’t have any clear advantage over GeoPackage, however it is unable to support raster data.\n\n\n1.2.2.7 PostGIS\n\n\n\n\n\nPostGIS logo\n\n\n\n\nPostGIS is an opensource database extender for PostrgeSQL. Essentially PostgreSQL is a database and PostGIS is an add on which permits spatial functions. The advantages of using PostGIS over a GeoPackage are that it allows users to access the data at the same time, can handle large data more efficiently and reduces processing time. In this example calculating the number of bars per neighbourhood in Leon, Mexico the processing time reduced from 1.443 seconds (SQLite) to 0.08 seconds in PostGIS. However, data stored in PostGIS is much harder to share, move or copy.\n\n\n1.2.2.8 What will I use\nThe variety of data formats can see a bit overwhelming. I still have to check how to load some of these data formats that I don’t use frequently. But don’t worry, most of the time you’ll be using shapefiles, GeoPackages or raster data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#general-data-flow",
    "href": "01_geographic_data.html#general-data-flow",
    "title": "1  Geographic information",
    "section": "1.3 General data flow",
    "text": "1.3 General data flow\nAs Grolemund and Wickham state in R for Data Science…\n\n“Data science is a huge field, and there’s no way you can master it by reading a single book.”\n\nHowever, a nice place to start is looking at the typical workflow of a data science (or GIS) project which you will see throughout these practicals, which is summarised nicely in this diagram produced by Dr. Julia Lowndes adapted from Grolemund and Wickham.\n\n\n\n\n\nUpdated from Grolemund & Wickham’s classis R4DS schematic, envisioned by Dr. Julia Lowndes for her 2019 useR! keynote talk and illustrated by Allison Horst. Source: Allison Horst data science and stats illustrations\n\n\n\n\nTo begin you have to import your data (not necessarily environmental) into R or some other kind of GIS to actually be able to do any kind of analysis on it.\nOnce imported you might need to tidy the data. This really depends on what kind of data it is and we cover this later on in the course. However, putting all of your data into a consistent structure will be very beneficial when you have to do analysis on it — as you can treat it all in the same way. Grolemund and Wickham state that data is tidy when “each column is a variable, and each row is an observation”, we cover this more in next week in the [Tidying data] section.\nWhen you have (or haven’t) tidied data you then will most likely want to transform it. Grolemund and Wickham define this as “narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means)”. However, from a GIS point of view I would also include putting all of your data into a similar projection, covered next week in [Changing projections] and any other basic process you might do before the core analysis. Arguably these processes could include things such as: clipping (cookie cutting your study area), buffering (making areas within a distance of a point) and intersecting (where two datasets overlap).\nTidying and transform = data wrangling. Remember from the introduction this could be 50-80% of a data science job!\n\n\n\n\n\ndplyr introduction graphic. Source: Allison Horst data science and stats illustrations\n\n\n\n\nAfter you have transformed the data the next best thing to do is visualise it — even with some basic summary statistics. This simple step will often let you look at your data in a different way and select more appropriate analysis.\nNext up is modelling. Personally within GIS i’d say a better term is processing as the very data itself is usually a computer model of reality. The modelling or processing section is where you conduct the core analysis (more than the basic analysis already mentioned) and try to provide an answer to your research question.\nFinally you have to communicate your study and outputs, it doesn’t matter how good your data wrangling, modelling or processing is, if your intended audience can’t interpret it, well, it’s pretty much useless.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#uk-spatial-geography",
    "href": "01_geographic_data.html#uk-spatial-geography",
    "title": "1  Geographic information",
    "section": "1.4 UK spatial geography",
    "text": "1.4 UK spatial geography\nIn this practical we are going to take some regular data (without any geometry) and join it to a spatial data set so we can map it!\nFirstly we need spatial data. It can be quite a daunting task to attempt to understand all of the boundaries that are in use in England and Wales….briefly:\nStatistical hierarchy\n\nStatistical hierarchy are units that census data is collected, the smallest being an output area with around 100 residents.\nOutput areas can be aggregated to Lower Super Output Areas (LSOAs) with between 1,000 and 3,000 residents. These can be further aggregated to Middle Super Output Areas (MSOAs).\nOutput areas and LSOAs typically fit within administrative electoral wards (below)…\nWards and MSOAs fit within local authority areas\n\n\n\n\n\n\n\n\nNesting areas\n\n\n\n\n\n\n\n\n\nNesting example\n\n\n\nNote that all the boundaries can change (e.g. Some LSOAs between the 2011 census and 2021 census moved). To account for this we can use lookup tables to match the new areas with the old ones.\n\nAdministarive hierarchy\n\nAdministrative areas are based on government areas and this depends on where you are in England….\n\n\n\n\n\n\n\n\nA Beginner’s Guide to UK Geography\n\n\n\nSome parts of England have a two tier structure. Counties take on expensive services - such as education and transport. Whilst local authority districts took on smaller services - such as planning permission, markets and local roads. Under all of this are electoral wards that have local Councillors…\nIn 1974 a two tier system of counties and districts was enacted across England and Wales. In urban areas these were metropolitan counties and metropolitan districts..\nBut in 1986 the metropolitan counties were removed (although still recognised) and the metropolitan districts were left as a single authority.\nFrom 1990 many of the tier structures (not in metropolitan areas) were combined into a single structure called Unitary Authorities, especially in medium-sized urban areas. However, some still retained the two tier structure.\n\nAn easy to read guide on census / administrative geography was produced by the London Borough of Tower Hamlets - skip to page 2 for a visual summary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#r-spatial-data-intro",
    "href": "01_geographic_data.html#r-spatial-data-intro",
    "title": "1  Geographic information",
    "section": "1.5 R Spatial data intro",
    "text": "1.5 R Spatial data intro\nR has a very well developed ecosystem of packages for working with Spatial Data. Early pioneers like Roger Bivand and Edzer Pebesma along with various colleagues were instrumental in writing packages to interface with some powerful open source libraries for working with spatial data, such as GDAL and GEOS. These were accessed via the rgdal and rgeos packages. The maptools package by Roger Bivand, amongst other things, allowed Shapefiles to be read into R. The sp package (along with spdep) by Edzer Pebesma was very important for defining a series of classes and methods for spatial data natively in R which then allowed others to write software to work with these formats. Many these original packages were retired (and superseded by the ones we will use today) at the end of 2023 as their maintainer Roger Bivand also retired. Other packages like raster advanced the analysis of gridded spatial data, while packages like classInt and RColorbrewer facilitated the binning of data and colouring of choropleth maps.\nWhilst these packages were extremely important for advancing spatial data analysis in R, they were not always the most straightforward to use — making a map in R could take quite a lot of effort and they were static and visually basic. However, more recently new packages have arrived to change this. Now leaflet enables R to interface with the leaflet javascript library for online, dynamic maps. ggplot2 which was developed by Hadley Wickham and colleagues radically changed the way that people thought about and created graphical objects in R, including maps, and introduced a graphical style which has been the envy of other software to the extent that there are now libraries in Python which copy the ggplot2 style!\n\n\n\n\n\nggplot2 introduction graphic. Source: Allison Horst data science and stats illustrations\n\n\n\n\nBuilding on all of these, the new tmap (Thematic Map) package has changed the game completely and now enables us to read, write and manipulate spatial data and produce visually impressive and interactive maps, very easily. In parallel, the sf (Simple Features) package is helping us re-think the way that spatial data can be stored and manipulated. It’s exciting times for geographic information / spatial data science!\n\n1.5.1 Spatial data projections\nSpatial data must be located somewhere on Earth and we need to represent this! We do this with Coordinate Reference Systems, shortened to CRS or often sometimes projections (although a projection is just one part of a coordinate reference system).\nProjection systems are mathematical formulas that specify how our data is represented on a map. These can either be call geographic coordinate reference systems or projected coordinate reference systems. The former treats data as a sphere and the latter as a flat object. You might come across phrases such as a resolution of 5 minutes or a resolution of 30 metres, which can be used to establish what kind of projection system has been used. Let me explain…\nA minute type of resolution (e.g. 5 minute resolution) is a geographic reference system that treats the globe as if it was a sphere divided into 360 equal parts called degrees (which are angular units). Each degree has 60 minutes and each minute has 60 seconds. Arc-seconds of latitude (horizontal lines in the globe figure below) remain almost constant whilst arc-seconds of longitude (vertical lines in the globe figure below) decrease in a trigonometric cosine-based fashion as you move towards the Earth’s poles…\n\n\n\n\n\nLatitude and Longitude. Source: ThoughtCo.\n\n\n\n\nThis causes problems as you increase or decrease latitude the longitudinal lengths alter…For example at the equator (0°, such as Quito) a degree is 111.3 km whereas at 60° (such as Saint Petersburg) a degree is 55.80 km …\nIn contrast a projected coordinate system is defined on a flat, two-dimensional plane (through projecting a spheroid onto a 2D surface) giving it constant lengths, angles and areas…\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula. Source: Lovelace et al. (2019) section 2.2\n\n\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula. Source: Lovelace et al. (2019) section 2.2\n\n\n\n\nKnowing this, if we want to conduct analysis locally (e.g. at a national level) or use metric (e.g. kilometres) measurements we need to be able to change the projection of our data or “reproject” it. Most countries and even states have their own projected coordinate reference system such as British National Grid in the above example…Note how the origin (0,0) is has moved from the centre of the Earth to the bottom South West corner of the UK, which has now been ironed (or flattened) out.\n\n\n\n\n\n\nImportant\n\n\n\nProjection rules\nUnits are angular (e.g. degrees, latitude and longitude) or the data is global = Geographic coordinate reference system\nUnits are linear (e.g. feet, metres) or data is at a local level (e.g. national, well the last one is not always true, but likely) = Projected coordinate reference system.\n\n\n\nYou might hear some key words about projections that could terrify you! Let’s break them down:\n\nEllipsoid (or spheroid) = size of shape of the Earth (3d)\nDatum = contains the point relationship (where the origin (0,0) of the map is) between a Cartesian coordinates (flat surface) and Earth’s surface. They can be local or geocentric (see below). They set the origin, the scale and orientation of the Coordiante Reference System (CRS).\nLocal datum = changes the Ellipsoid to align with a certain location on the surface (e.g. BNG that uses the OSGB36 datum). A local datum is anything that isn’t the centre of the Earth.\nGeocentric datum = the centre is equal to the Earth’s centre of gravity (e.g. WGS84).\nGeodetic datum = global datum (see above for datum meaning) for representing features (e.g. points and polygons) on earth\nGeodesy (from which we get Geodetic) = measuring Earth’s shape and features (e.g. gravity field).\nCoordinate reference system (CRS) = Formula that defines how the 2D map (e.g. on your screen or a paper map) relates to the 3D Earth. Sometimes called a spatial Reference System (SRS). It also stores the datum information.\n\n\n\n\n\n\n\nTip\n\n\n\nTake home message\nWhen you do analysis on multiple datasets make sure they are all use the same Coordinate Reference System.\nIf it’s local (e.g. city of country analysis) then use a local projected CRS where possible.\n\n\n\n\n1.5.2 Data download\nOk, after all that theory we can start downloading data! In this case we will be joining the “health in general” question from the 2021 census to LSOAs in London (although you could select any area).\nMake a new R project and put this data into a data folder\nThe health data be accessed either from:\n\nThe ONS. Note make sure you have selected the right area (in our case LSOA).\nThe London data store. Note this is excel data and the function we would need is read_excel(excel document, sheet number)\n\nOur spatial data can also be accessed from either:\n\nThe ONS\nThe London data store\n\nIn this example i will use the health data from the ONS and spatial data from the London Datastore.\nFirst, load the packages we need:\n\nlibrary(sf)\nlibrary(tidyverse)\n\nThen our data…\n\n# spatial data\n\nLSOAs &lt;- sf::st_read(\"prac1_data/statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp\")\n\n# health data\n  \nhealth &lt;- readr::read_csv(\"prac1_data/TS037-2021-3-filtered-2024-01-24T17_28_55Z.csv\")  \n\nTo check our spatial data let’s make a quick map with the thematic maps package (tmap) this works on the grammar of graphics (famous from ggplots), similar to the grammar of data manipulation (tidyverse) it works on a layered approach. Here we specify the dataset and then how we want to style it..In the most basic form…\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n# plot or view - view will make it interactive\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n# load the sf object\ntm_shape(LSOAs) +\n  # style it with polygons.\n  tm_polygons(col = NA, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n1.5.3 Wrangle\nBefore progressing it’s also good practice to standardise our column names…we can do so with the janitor package…\n\n\n\n\n\njanitor::clean_names() example. Source: Allison Horst data science and stats illustrations\n\n\n\n\n\nlibrary(janitor)\n\nLSOAs &lt;- janitor::clean_names(LSOAs)\n\nhealth &lt;- janitor::clean_names(health)\n\nNext we need to join the health data to the spatial data…to do so need to identify a unique column in each dataset to perform the join on, such as a code for the LSOAs (e.g. lsoa11cd and lower layer super output areas code ).\nOnce we have the code we can select a join type…\n\n\n\n\n\nSQL join types. Source: SQL Join Diagram, dofactory\n\n\n\n\nTypically in spatial analysis we use a left join - this retains everything in th left data (which is our spatial data set) and joins data from the right only where there are matches\n\n\n\n\n\ndplyr::left_join() example. Source: Tidy explain by Garrick Aden‑Buie\n\n\n\n\n\nIf there are multiple matches then a new row is created (e.g. If there were two health rows for a single LSOA)\nIf there are no matches then the data is dropped (e.g. the LSOAs not in London), but the polygons (the left dataset) are retained.\nNote, if this were the case i could filter out the London LSOAs based on the lad11cd column starting with E09, something like.. filter(str_detect(lad11cd, \"^E09\")) or join the data and then filter based on NAs\n\n\n\n\n\n\ndplyr::left_join() example. Source: Tidy explain by Garrick Aden‑Buie\n\n\n\n\n\njoined_data &lt;- LSOAs %&gt;%\n  left_join(., \n            health,\n            by = c(\"lsoa11cd\" = \"lower_layer_super_output_areas_code\"))\n\n\n\n1.5.4 Long vs wide data\nYou will get a warning saying that each row in x (the spatial data) was expected to match just 1 y row. However, our health data is long data (also called tidy data). This differs from “regular” wide data as…\n\nEach variable (all values that have the same attribute, e.g. height, temperature, duration, weeks) must form its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nNote, see Wickham’s paper for worked examples and definition of variables, from page 3.\n\n\n\n\n\nThis figure is taken directly from Grolemund and Wickham (2017) Chapter 12.Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. Source: KSK analytics\n\n\n\n\nTypically in GIS we need our data messy (or wide) where the variables have their own column and each row is an area.\nTo so do first we must make the data into a tibble..\n\n\n\n\n\nData Object Type and Structure. Source: Exploratory Data Analysis in R, Gimond 2022\n\n\n\n\nWe should reflect on data types in R, which will influence the structure we select. Note a tibble is very similar (essentially the same!) to a dataframe, except you are provided with additional information when printing.\n\nlibrary(tidyr)\n\njoined_data_wide &lt;- joined_data %&gt;%\n  as_tibble(.)%&gt;%\n  select(lsoa11cd, general_health_6_categories, observation, usualres, hholdres, popden)%&gt;%\n  # move the general health from 1 column to a column for each category\n  tidyr::pivot_wider(.,\n#    id_cols=1:8,\n    names_from = general_health_6_categories,\n    values_from = observation)%&gt;%\n    clean_names(.)\n\nWhen we make the tibble we lose the geometry column and so our data becomes non spatial again…really we could have done our wrangling first and then conducted a join! This will create a bit of a mess with columns too (as we already have some), so we will need to select the ones we want…\n\njoined_data_wide_joined &lt;- LSOAs %&gt;%\n  left_join(., \n            joined_data_wide,\n            by = c(\"lsoa11cd\" = \"lsoa11cd\"))%&gt;%\n    select(lsoa11cd, msoa11cd, usualres.x, hholdres.x, popden.x, does_not_apply, very_good_health, good_health, fair_health, bad_health, very_bad_health)\n\n\n\n1.5.5 Map\nOnce we have the wide data we can compute other metrics - this is especially important for mapping as we must never map count data, unless the spatial units are the same size (e.g. hexagons). Instead we should normalise our data using some kind of common denominator….for example percent of usual residents with bad health…where the number of usual residents will vary across the spatial units.\n\njoined_data_wide_joined_map &lt;- joined_data_wide_joined%&gt;%\n  mutate(percent_very_bad = (very_bad_health/usualres.x)*100)%&gt;%\n  mutate(percent_very_bad = round(percent_very_bad, digits=2))\n\nMake a basic map!\n\n# select the sf object to map\ntm1 &lt;- tm_shape(joined_data_wide_joined_map) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\")\n\ntm1\n\n\n\n\n\n\n\n\nThere are some issues with our map that we can resolve…\n\nThe legend is covering the data and is using the object name (with underscores)\nNo scale bar\nThe LSOAs are fairly small and so it can be challenging to interpret them\n\n\nlibrary(tmap)\n\ntm1 &lt;- tm_shape(joined_data_wide_joined_map) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\",\n              # how the data should be divided\n              style=\"jenks\",\n              # legend title\n              title = \"\")+\n  \n  tm_compass(position = c(\"left\", \"top\"), size = 2)+\n  \n  tm_layout(main.title=\"% of population with very bad health\",\n          legend.outside=FALSE, \n          frame = TRUE, \n          legend.position = c(0.8,0),\n          legend.text.size = 1)+\n\n  # tm_layout(legend.outside.size = FALSE, \n  #            legend.position= c(\"right\", \"top\"))+\n  tm_scale_bar(position=c(0,0.03), text.size = 1) +\n  \n  tm_credits(\"Data source: ONS and London Data store\",\n          position=c(0,0), \n          size = 0.8, \n          align=\"left\") \n\n\ntm1\n\n\n\n\n\n\n\n\nTo export the map…\n\ntmap_save(tm1, 'very bad health.png')\n\nThis hasn’t solved the LSOA issue - whereby the map is challenging to read due to the spatial units used. We can consider aggregating our units to MSOA as that column is provided within the LSOA data…\nTo do so we’d need to:\n\nAggregate our current data\nLoad the MSOA spatial data, then join and map as above.\n\nTo aggregate the data we use a function called group_by() which is always followed by summarise(). Group by places our data into groups based on a selected column (e.g. MSOA) and then summarises the data for each group (e.g. number of people with very bad health)\n\nMSOA_data &lt;- joined_data_wide_joined_map %&gt;%\n  as_tibble(.)%&gt;%\n  select(-lsoa11cd, -geometry, -percent_very_bad)%&gt;%\n  group_by(msoa11cd)%&gt;%\n  summarise_all(sum)\n\nIn the above code select(-variable) means drop that variable, this has allowed me to use the summarise_all() function as opposed to just summarise(). Now each column is aggregated to MSOA!\nCalculate the percentages\n\nMSOA_data_percent &lt;- MSOA_data%&gt;%\n  mutate(percent_very_bad = (very_bad_health/usualres.x)*100)%&gt;%\n  mutate(percent_very_bad = round(percent_very_bad, digits=2))\n\nRead in the MSOA spatial data\n\nMSOAs &lt;- sf::st_read(\"prac1_data/statistical-gis-boundaries-london/ESRI/MSOA_2011_London_gen_MHW.shp\")%&gt;%\n  clean_names(.)\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_25\\spatial analysis of public health\\spatial-analysis-of-public-health-data-25\\prac1_data\\statistical-gis-boundaries-london\\ESRI\\MSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nJoin…\n\nMSOA_joined &lt;- MSOAs %&gt;%\n  left_join(., \n            MSOA_data_percent,\n            by = c(\"msoa11cd\" = \"msoa11cd\"))%&gt;%\n    select(msoa11cd, msoa11cd, usualres, hholdres, popden, does_not_apply, very_good_health, good_health, fair_health, bad_health, very_bad_health, percent_very_bad)\n\nMap the MSOAs\n\ntm2 &lt;- tm_shape(MSOA_joined) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\",\n              # how the data should be divided\n              style=\"jenks\",\n              # legend title\n              title = \"\")+\n  \n  tm_compass(position = c(\"left\", \"top\"), size = 2)+\n  \n  tm_layout(main.title=\"% of population with very bad health\",\n          legend.outside=FALSE, \n          frame = TRUE, \n          legend.position = c(0.8,0),\n          legend.text.size = 1)+\n\n  # tm_layout(legend.outside.size = FALSE, \n  #            legend.position= c(\"right\", \"top\"))+\n  tm_scale_bar(position=c(0,0.03), text.size = 1) +\n  \n  tm_credits(\"Data source: ONS and London Data store\",\n          position=c(0,0), \n          size = 0.8, \n          align=\"left\") \n\n\ntm2\n\n\n\n\n\n\n\n\nPlot them together…\n\nt=tmap_arrange(tm1, tm2, ncol=2)\n\nt",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html",
    "href": "02_point_patterns.html",
    "title": "2  Point patterns and autocorrelation",
    "section": "",
    "text": "2.1 Learning outcomes\nBy the end of this practical you should be able to:\nToday’s lecture is available online…at this URL: https://andrewmaclachlan.github.io/Spatial-analysis-of-public-health-data-24-points-and-autocorrelation/#1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#learning-outcomes",
    "href": "02_point_patterns.html#learning-outcomes",
    "title": "2  Point patterns and autocorrelation",
    "section": "",
    "text": "Describe and evaluate methods for analysing spatial patterns\nExecute data cleaning and manipulation appropriate for analysis\nDetermine the locations of spatial clusters using point pattern analysis methods\nInvestigate the degree to which values at spatial points are similar (or different) to each other\nInterpret the meaning of spatial autocorrelation in spatial data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#introduction",
    "href": "02_point_patterns.html#introduction",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nToday you will learn how to begin to analyse patterns in spatial data with points and spatially continuous observations.\nThe questions we want to answer are:\n\nFor any given London Ward, are Pharmacies distributed randomly or do they exhibit some kind of dispersed or clustered pattern\nAre the values (in this case the density of pharmacies) similar (or dissimilar) across the wards of London.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#data",
    "href": "02_point_patterns.html#data",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.3 Data",
    "text": "2.3 Data\nLoad our packages\n\nlibrary(spatstat)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(here)\nlibrary(sf)\nlibrary(RColorBrewer)\nlibrary(spdep)\n\nGet the pharmacy data: https://datashare.ed.ac.uk/handle/10283/2501 and load it\n\npharmacy &lt;- st_read(\"prac2_data/PharmacyEW/PharmacyEW.shp\")%&gt;%\n  #remove any duplicates\n  distinct()\n\nReading layer `PharmacyEW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_25\\spatial analysis of public health\\spatial-analysis-of-public-health-data-25\\prac2_data\\PharmacyEW\\PharmacyEW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 10589 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 137070 ymin: 19187 xmax: 655138 ymax: 653253\nProjected CRS: OSGB36 / British National Grid\n\n#check CRS\nst_crs(pharmacy)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9.01,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n#check with a plot\ntm_shape(pharmacy) +\n  tm_dots(col = \"blue\")\n\n\n\n\n\n\n\n\nGet the London ward data and load it https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london\n\nwards&lt;- st_read(\"prac2_data/statistical-gis-boundaries-london/statistical-gis-boundaries-london/ESRI/London_Ward_CityMerged.shp\")%&gt;%\n  st_transform(.,27700)\n\nReading layer `London_Ward_CityMerged' from data source \n  `prac2_data/statistical-gis-boundaries-london/statistical-gis-boundaries-london/ESRI/London_Ward_CityMerged.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 625 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n#check CRS\n#st_crs(wards)\n\nCheck the data\n\n#check with a plot\ntm_shape(wards) +\n  tm_polygons(col = NA)\n\n\n\n\n\n\n\n\n\n2.3.1 Wrangle data\nAs we can see above the pharmacy data is for the whole of the UK and we are just interested in London. So, we need to spatially subset our points within our study area…\nHere, the second operator is blank , , - this controls which columns are kept, although I’d rather keep all of them and manipulate with the tidyverse (e.g. select(the columnns i want).\n\npharmacysub &lt;- pharmacy[wards, , op=st_within]\n\n#check with a plot\ntm_shape(wards) +\n  tm_polygons(col = NA, alpha=0.5)+\ntm_shape(pharmacysub) +\n  tm_dots(col=\"blue\")\n\n\n\n\n\n\n\n\nWhen we spatial subset data like this there are different topological relations we can specify. The default is intersects, but we could also use pharmacysub &lt;- pharmacy[wards, , op=st_within], with the operator or op set to st_within, to identify points completely within the borough outline, or a variety of other options such as st_overlaps, st_touches, st_contains, st_disjoint. Any possible topological relationship you can think of a function will exist for it…visually this looks like the image below, where each tick denotes the relations that apply to the polygons. Note, that in several cases multiple topological relations would work.\n\n\n\n\n\nTopological relations between vector geometries. Source: Lovelace et al. 2022\n\n\n\n\nWe can also just use the function which will have the indices of where they intersect.\n\n# add sparse=false to get the complete matrix.\nintersect_indices &lt;-st_intersects(wards, pharmacy)\n\nIf you have used a graphic user interface GIS before, this is the same as select by location (e.g. select by location in QGIS), and as using filter from dplyr is the same as select by attribute.\n\n\n\n\n\n\nTip\n\n\n\nWhat is the difference between intersects and within for points like ours?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#point-patterns",
    "href": "02_point_patterns.html#point-patterns",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.4 Point patterns",
    "text": "2.4 Point patterns\nFor point pattern analysis we need a point pattern object (ppp)…within an observation window. This is specific to the spatstat package as we can’t do this with sf, SpatialPolygonsDataFrames or SpatialPointsDataFrames.\nFor this example i will set the boundary to London so we can see the points\n\nboroughs &lt;- st_read(\"prac2_data/statistical-gis-boundaries-london/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\")%&gt;%\n  st_transform(.,27700)\n\nReading layer `London_Borough_Excluding_MHW' from data source \n  `prac2_data/statistical-gis-boundaries-london/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 33 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n#now set a window as the borough boundary\nwindow &lt;- as.owin(boroughs)\nplot(window)\n\n\n\n\n\n\n\n#create a sp object\npharmacysubsp&lt;- pharmacysub %&gt;%\n  as(., 'Spatial')\n#create a ppp object\npharmacysubsp.ppp &lt;- ppp(x=pharmacysubsp@coords[,1],\n                          y=pharmacysubsp@coords[,2],\n                          window=window)\n\npharmacysubsp.ppp %&gt;%\n  plot(.,pch=16,cex=0.5, \n              main=\"Pharmacies in London\")\n\n\n\n\n\n\n\n\n\n2.4.1 Quadrat analysis\nSo as you saw in the lecture, we are interested in knowing whether the distribution of points in our study area differs from ‘complete spatial randomness’ — CSR. That’s different from a CRS! Be careful!\nThe most basic test of CSR is a quadrat analysis. We can carry out a simple quadrat analysis on our data using the quadrat count() function in spatstat. Note, I wouldn’t recommend doing a quadrat analysis in any real piece of analysis you conduct, but it is useful for starting to understand the Poisson distribution…\n\n#First plot the points\n\nplot(pharmacysubsp.ppp,\n     pch=16,\n     cex=0.5, \n     main=\"Pharmacies\")\n\n#now count the points in that fall in a 20 x 20\n#must be run all together otherwise there is a plot issue\n\npharmacysubsp.ppp %&gt;%\n  quadratcount(.,nx = 20, ny = 20)%&gt;%\n    plot(., add=T, col=\"red\")\n\n\n\n\n\n\n\n\nWe want to know whether or not there is any kind of spatial patterning associated with pharmacies in areas of London. If you recall from the lecture, this means comparing our observed distribution of points with a statistically likely (Complete Spatial Random) distribution, based on the Poisson distribution.\nUsing the same quadratcount() function again (for the same sized grid) we can save the results into a table:\n\n#run the quadrat count\nqcount &lt;- pharmacysubsp.ppp %&gt;%\n  quadratcount(.,nx = 20, ny = 20) %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::count(var1=Freq)%&gt;%\n  dplyr::rename(freqquadratcount=n)\n\nOK, so we now have a frequency table — next we need to calculate our expected values. The formula for calculating expected probabilities based on the Poisson distribution is:\n\\[Pr= (X =k) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!}\\] where:\n\nx is the number of occurrences\nλ is the mean number of occurrences\ne is a constant- 2.718\n\n\nsums &lt;- qcount %&gt;%\n  #calculate the total blue plaques (Var * Freq)\n  mutate(total = var1 * freqquadratcount) %&gt;%\n  # then the sums\n  dplyr::summarise(across(everything(), sum))%&gt;%\n  dplyr::select(-var1) \n\nlambda&lt;- qcount%&gt;%\n  #calculate lambda - sum of freq count / sum of total plaques\n  mutate(total = var1 * freqquadratcount)%&gt;%\n  dplyr::summarise(across(everything(), sum)) %&gt;%\n  mutate(lambda=total/freqquadratcount) %&gt;%\n  dplyr::select(lambda)%&gt;%\n  pull(lambda)\n\nCalculate expected using the Poisson formula from above \\(k\\) is the number of pharmacies counted in a square and is found in the first column of our table…\n\nqcounttable &lt;- qcount %&gt;%\n  #Probability of number of plaques in quadrant using the formula \n  mutate(pr=((lambda^var1)*exp(-lambda))/factorial(var1))%&gt;%\n  #now calculate the expected counts based on our total number of plaques\n  #and save them to the table\n  mutate(expected= (round(pr * sums$freqquadratcount, 0)))\n\nPlot them\n\nqcounttable_long &lt;- qcounttable %&gt;% \n  pivot_longer(c(\"freqquadratcount\", \"expected\"), \n               names_to=\"countvs_expected\", \n               values_to=\"value\")\n\nggplot(qcounttable_long, aes(var1, value)) +\n  geom_line(aes(colour = countvs_expected ))\n\n\n\n\n\n\n\n\nCheck for association between two categorical variables - we are looking to see if our freqneucy is similar to the expected (which is random)\nTo check for sure, we can use the quadrat.test() function, built into spatstat. This uses a Chi Squared test to compare the observed and expected frequencies for each quadrant (rather than for quadrant bins, as we have just computed above).\nA Chi-Squared test determines if there is an association between two categorical variables. The higher the Chi-Squared value, the greater the difference.\nIf the p-value of our Chi-Squared test is &lt; 0.05, then we can reject a null hypothesis that says “there is no pattern - i.e. complete spatial randomness - in our data” (think of a null-hypothesis as the opposite of a hypothesis that says our data exhibit a pattern). What we need to look for is a value for p &gt; 0.05. If our p-value is &gt; 0.05 then this indicates that we have CSR and there is no pattern in our points. If it is &lt; 0.05, this indicates that we do have clustering in our points.\n\nteststats &lt;- quadrat.test(pharmacysubsp.ppp, nx = 20, ny = 20)\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\nChi square with a p value &lt; 0.05 therefore some clustering…but from the plot, this was expected\n\n\n2.4.2 Ripley K\nOne way of getting around the limitations of quadrat analysis is to compare the observed distribution of points with the Poisson random model for a whole range of different distance radii. This is what Ripley’s K function computes. We can conduct a Ripley’s K test on our data very simply with the spatstat package using the kest() function.\nRipley’s K is defined as…\n\\[K(r) = \\lambda^{-1} \\sum{i}\\sum{j}\\frac{I(d_ij&lt;r)}{n}\\]\n\nIn English: Ripley’s K value for any circle radius \\(r\\) =\n\nThe average density of points for the entire study region (of all locations) \\(\\lambda = (n/ \\Pi r^2))\\)\nMultiplied by the sum of the distances \\(d_ij\\) between all points within that search radius, see Dixon page 2 and Amgad et al. 2015\nDivided by the total number of points, n\nI = 1 or 0 depending if \\(d_ij &lt; r\\)\n\n\nThe plot for K has a number of elements that are worth explaining. First, the Kpois(r) line in Red is the theoretical value of K for each distance window (r) under a Poisson assumption of Complete Spatial Randomness. The Black line is the estimated values of K accounting for the effects of the edge of the study area.\nHere, the correction specifies how points towards the edge are dealt with, in this case, border means that points towards the edge are ignored for the calculation but are included for the central points. Section 2.1, here explains the different options.\nWhere the value of K falls above the line, the data appear to be clustered at that distance. Where the value of K is below the line, the data are dispersed…\n\nK &lt;- pharmacysubsp.ppp %&gt;%\n  Kest(., correction=\"border\") %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nThis was sort of expected too due to our previous analysis - suggesting that there is clustering throughout the points.\n\n\n2.4.3 DBSCAN\nQuadrat and Ripley’s K analysis are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us WHERE in our area of interest the clusters are occurring. To discover this we need to use alternative techniques. One popular technique for discovering clusters in space (be this physical space or variable space) is DBSCAN. For the complete overview of the DBSCAN algorithm, read the original paper by Ester et al. (1996) or consult the wikipedia page\nDBSCAN requires you to input two parameters: 1. Epsilon - this is the radius within which the algorithm with search for clusters 2. MinPts - this is the minimum number of points that should be considered a cluster\nWe could use the output of Ripley’s K to inform Epsilon or alternatively we can use kNNdistplot() from the dbscan package to find a suitable eps value based on the ‘knee’ in the plot…\nkNNdistplot() take the average distance to all neighbours then ploits the values in ascending order, where the “knee” is indicates a sudden increase in distance to neighbours.\nFor MinPts we typically start with 4. But, try increasing the value of k in kNNdistplot you will notice as you increase it the knee becomes less obvious.\n\nIf we have MinPts too low we have a massive cluster\nIf we have MinPts too high we have a small single cluster\n\n\n#first extract the points from the spatial points data frame\npharmacysub_coords &lt;- pharmacysub %&gt;%\n  st_coordinates(.)%&gt;%\n  as.data.frame()\n\npharmacysub_coords%&gt;%\n  dbscan::kNNdistplot(.,k=20)\n\n\n\n\n\n\n\n\nI started with an eps of 1600 and a minpts of 20..however…the large eps means that the city centre has a massive cluster…this isn’t exactly what i wanted to pull out. Instead i want to identify local clusters of pharmacies so try reducing the eps to 500 and the minpts to 5\nOPTICS will let us remove the eps parameter but running every possible value, however, minpts is always meant to be domain knowledge\nDepending on your points it might be possible to filter the values you aren’t interested in - this isn’t the case here, but for example stop and search data or flytipping could be filtered (well, depending on the extra data within the columns)\n\n#now run the dbscan analysis\ndb &lt;- pharmacysub_coords %&gt;%\n  fpc::dbscan(.,eps = 500, MinPts = 5)\n\n#now plot the results\nplot(db, pharmacysub_coords, main = \"DBSCAN Output\", frame = F)\nplot(wards$geometry, add=T)\n\n\n\n\n\n\n\n\nOur new db object contains lots of info including the cluster each set of point coordinates belongs to, whether the point is a seed point or a border point etc. We can get a summary by just calling the object. We can now add this cluster membership info back into our dataframe\n\npharmacysub_coords&lt;- pharmacysub_coords %&gt;%\n  mutate(dbcluster=db$cluster)\n\nNow create a ggplot2 object from our data\n\npharmacysub_coordsgt0 &lt;- pharmacysub_coords %&gt;%\n  filter(dbcluster&gt;0)\n\ndbplot &lt;- ggplot(data=wards)+\n  geom_sf()+\n  geom_point(data=pharmacysub_coordsgt0, \n                 aes(X,Y, colour=dbcluster, fill=dbcluster))\n#add the points in\n\ndbplot + theme_bw() + coord_sf()\n\n\n\n\n\n\n\n\nNow, this identifies where we have clustering based on our criteria but it doesn’t show where we have similar densities of pharmacies.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#spatial-autocorrelation",
    "href": "02_point_patterns.html#spatial-autocorrelation",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.5 Spatial Autocorrelation",
    "text": "2.5 Spatial Autocorrelation\nIn this section we are going to explore patterns of spatially referenced continuous observations using various measures of spatial autocorrelation. Spatial autocorrelation is a measure of similarity between nearby data. We need to add all the points to the London wards then compute a density per ward could also use population here too! or some other data that can give us meaningful comparisons for our variable of interest.\n\n2.5.1 Wrangle data\nTo do so we need to use a spatial join!\nThis is similar to the the joins (e.g. left joins) we explored with attribute data but here we just want to join datasets together based on their geometry and keep all their attribute data, this is useful in the code below where i want to join the pharmacies to the LSOA data\nThe output will be a massive dataset where each pharmacy will be a new row and will retain the attributes of the pharmacy data but also append the attribute of the LSOA.\nThe spatial join function,st_join(), defaults to a left join, so in this case the LSOA data is the left dataset and all the right data has been appended to it. If the left data (LSOA) had no matches (so no pharmacies) it would still appear in the final dataset. The default argument for this is st_intersects but we could also use other topological relationship functions such as st_within() instead…\n\nexample&lt;-st_intersects(wards, pharmacysub)\n\nexample\n\nSparse geometry binary predicate list of length 625, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: (empty)\n 2: 61, 65\n 3: 53, 59\n 4: 45, 54, 68\n 5: 44, 48, 49, 62\n 6: (empty)\n 7: 60, 66\n 8: 57\n 9: 55\n 10: 43, 56\n\n\nHere the polygon with the ID of 7, Chessington North and Hook, has two pharmacies within it…we can check this with st_join (or using QGIS by opening the data).\nBut note the ID column added is different to the ID of the data…open pharmacysub from the environment window and you will see the IDs that were returned in st_intersects(). The new IDs above take the data and apply 1 to n, where n is the end of the data. So if we subset our pharmacies on row 60 and 66 it will match our result here.\n\ncheck_example &lt;- wards%&gt;%\n  st_join(pharmacysub)%&gt;%\n  filter(GSS_CODE==\"E05000404\")\n\ncheck_example\n\nSimple feature collection with 2 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 517153.7 ymin: 164037.4 xmax: 519553 ymax: 165447.1\nProjected CRS: OSGB36 / British National Grid\n                          NAME  GSS_CODE HECTARES NONLD_AREA LB_GSS_CD\n7   Chessington North and Hook E05000404   192.98          0 E09000021\n7.1 Chessington North and Hook E05000404   192.98          0 E09000021\n                 BOROUGH POLY_ID  ID      PCTName         NamePharm\n7   Kingston upon Thames   50530 130 Kingston PCT Alliance Pharmacy\n7.1 Kingston upon Thames   50530 136 Kingston PCT Alliance Pharmacy\n           Address1    Address2    Address3 Address4 PostCode LSOAName Easting\n7   11 North Parade Chessington      Surrey     &lt;NA&gt;  KT9 1QL     &lt;NA&gt;  518483\n7.1 4 Arcade Parade        Hook Chessington   Surrey  KT9 1AB     &lt;NA&gt;  518015\n    Northing                       geometry\n7     164147 POLYGON ((517175.3 164077.3...\n7.1   164508 POLYGON ((517175.3 164077.3...\n\n\nNow we just take the length of each list per polygon and add this as new column…\n\npoints_sf_joined &lt;- wards%&gt;%\n  mutate(n = lengths(st_intersects(., pharmacysub)))%&gt;%\n  janitor::clean_names()%&gt;%\n  #calculate area\n  mutate(area=st_area(.))%&gt;%\n  #then density of the points per ward\n  mutate(density=n/area)\n\nNow map density\n\npoints_sf_joined&lt;- points_sf_joined %&gt;%                    \n  group_by(gss_code) %&gt;%         \n  summarise(density = first(density),\n          name  = first(gss_code))\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density\",\n        style=\"jenks\",\n        palette=\"PuOr\",\n        midpoint=NA)\n\n\n\n\n\n\n\n\nSo, from the map, it looks as though we might have some clustering of pharmacies in the centre of London and a few other places, so let’s check this with Moran’s I and some other statistics.\nAs we saw in the session we need to create a spatial weight matrix…to do so we need centroid points first…to then compute the neighbours of each centroid…\n\ncoordsW &lt;- points_sf_joined%&gt;%\n  st_centroid()%&gt;%\n  st_geometry()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n#check, alpha is transparency \ntm_shape(points_sf_joined) +\n    tm_polygons(alpha=0.1)+\ntm_shape(coordsW) +\n  tm_dots(col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n2.5.2 Weight matrix\nNow we need to generate a spatial weights matrix (remember from the lecture). We’ll start with a simple binary matrix of queen’s case neighbours (otherwise known as Contiguity edges corners). This method means that polygons with a shared edge or a corner will be included in computations for the target polygon…A spatial weight matrix represents the spatial element of our data, this means we are trying to conceptualize and model how parts of the data are linked (or not linked) to each other spatially, using rules that we will set.\nIf the features share a boundary they are contiguous, this can also be classed as only common boundaries — a rook (like chess a rook can move forwards or side wards) or any point in common (e.g. corners / other boundaries) — a queen (like chess a queen can move forwards, backwards or on a diagonal).\nAlternatively instead of using contiguous relationships you can use distance based relationships. This is frequently done with k nearest neighbours in which k is set to the closest observations. e.g. K=3 means the three closest observations.\nIn the first instance we must create a neighbours list — which is a list of all the neighbours. To do so we will use polygon to neigbhour function poly2nb() with the argument queen=T saying we want a to use Queens case. Let’s see a summary of the output\n\n#create a neighbours list\nlward_nb &lt;- points_sf_joined %&gt;%\n  poly2nb(., queen=T)\n\nHave a look at the summary of neighbours - average is 5.88\n\nsummary(lward_nb)\n\nNeighbour list object:\nNumber of regions: 625 \nNumber of nonzero links: 3680 \nPercentage nonzero weights: 0.94208 \nAverage number of links: 5.888 \nLink number distribution:\n\n  1   2   3   4   5   6   7   8   9  10  11  12 \n  1   4  15  72 162 182 112  55  14   4   2   2 \n1 least connected region:\n380 with 1 link\n2 most connected regions:\n313 612 with 12 links\n\n\nplot them - we can’t use tmap as it isn’t the right class (e.g. it’s not an sf object)\n\nplot(lward_nb, st_geometry(coordsW), col=\"red\")\n#add a map underneath\nplot(points_sf_joined$geometry, add=T)\n\n\n\n\n\n\n\n\nNext take our weight list and make it into a matrix through the neigbhour to matix function (nb2mat) …style here denotes the weight type:\n\nB is the basic binary coding (1/0)\nW is row standardised (sums over all links to n)\nC is globally standardised (sums over all links to n)\nU is equal to C divided by the number of neighbours (sums over all links to unity)\nS is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\n\n\n#create a spatial weights matrix from these weights\nlward_lw &lt;- lward_nb %&gt;%\n  nb2mat(., style=\"W\",  zero.policy=TRUE)\n\nI have used row…based on the lecture what should the value of the weights sum to?\n\nsum(lward_lw)\n\n[1] 625\n\n\n\n\n2.5.3 Moran’s I\nMoran’s I requires a spatial weight list type object as opposed to matrix, this is simply…\n\nlward_lw &lt;- lward_nb %&gt;%\n  nb2listw(., style=\"W\",  zero.policy=TRUE)\n\nNow let’s run Moran’s I. This test tells us whether the values at neighbouring sites are similar to the target site (giving a Moran’s I close to 1) or the value of the target is different to the neighbours (close to -1)\n\ni_lWard_global_density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  moran.test(., lward_lw, zero.policy = TRUE)\n\nThe argument zero.policy = TRUE allows neighbours with no values.\n\n\n2.5.4 Geary’s C\nGeary’s C tells us whether similar values or dissimilar values are clustering.\nIt falls between 0 and 2; 1 means no spatial autocorrelation, &lt;1 - positive spatial autocorrelation or similar values clustering, &gt;1 - negative spatial autocorreation or dissimilar values clustering)\n\nc_lward_global_density &lt;- \n  points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  geary.test(., lward_lw, zero.policy = TRUE)\n\n\n\n2.5.5 Getis Ord General G\nGetis Ord General G…? This tells us whether high or low values are clustering. If G &gt; Expected = High values clustering; if G &lt; expected = low values clustering\n\ng_lward_global_density &lt;- \n  points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  globalG.test(., lward_lw, zero.policy = TRUE)\n\nWarning in globalG.test(., lward_lw, zero.policy = TRUE): Binary weights\nrecommended (especially for distance bands)\n\n\nBased on the results write down what you can conclude here….",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#local-indicies-of-spatial-autocorrelation",
    "href": "02_point_patterns.html#local-indicies-of-spatial-autocorrelation",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.6 Local Indicies of Spatial Autocorrelation",
    "text": "2.6 Local Indicies of Spatial Autocorrelation\n\n2.6.1 Moran’s I\nWe can now also calculate local versions of the Moran’s I statistic (for each Ward) and a Getis Ord \\(G_{i}^{*}\\) statistic to see where we have hot-spots…\nLocal Moran’s I is:\n\nThe difference between a value and neighbours * the sum of differences between neighbours and the mean\nWhere the the difference between a value and neighbours is divided by the standard deviation (how much values in neighbourhood vary about the mean)\n\nIt returns several columns, of most interest is the Z score. A Z-score is how many standard deviations a value is away (above or below) from the mean. This allows us to state if our value is significantly different than expected value at this location considering the neighours.\nWe are comparing our value of Moran’s I to that of an expected value (computed from a separate equation that uses the spatial weight matrix, and therefore considers the neighbouring values). We are expecting our value of Moran’s I to be in the middle of the distribution of the expected values. These expected values follow a normal distribution, with the middle part representing complete spatial randomness. This is typically between &lt; -1.65 or &gt; +1.65 standard deviations from the mean\nThe null hypothesis is always there is complete spatial randomness. A null hypothesis means:\n\nno statistical significance exists in a set of given observations\n\nIf our value is towards the tails of the distribution then it is unlikely that the value is completely spatially random and we can reject the null hypothesis…as it is not what we expect at this location.\nIn the example where we use a z-score of &gt;2.58 or &lt;-2.58 we interpret this as…\n…&gt; 2.58 or &lt;-2.58 standard deviations away from the mean are significant at the 99% level…this means there is a &lt;1% chance that autocorrelation is not present\nThe Global vs location spatial autocorrelation resource goes through the specific formulas here, but the most important parts are knowing\n\nWhat we are comparing values to in Local Moran’s I\nWhat the results mean\nWhy the results could be important\n\n\ni_lward_local_density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  localmoran(., lward_lw, zero.policy = TRUE)%&gt;%\n  as_tibble()\n\nThis outputs a table, so we need to append that back to our sf of the wards to make a map…\n\npoints_sf_joined &lt;- points_sf_joined %&gt;%\n    mutate(density_i =as.numeric(i_lward_local_density$Ii))%&gt;%\n    mutate(density_iz =as.numeric(i_lward_local_density$Z.Ii))%&gt;%\n    mutate(p =as.numeric(i_lward_local_density$`Pr(z != E(Ii))`))\n\nWe’ll set the breaks manually based on the rule that data points:&gt;2.58 or &lt;-2.58 standard deviations away from the mean are significant at the 99% level (&lt;1% chance that autocorrelation not present); &gt;1.96 - &lt;2.58 or &lt;-1.96 to &gt;-2.58 standard deviations are significant at the 95% level (&lt;5% change that autocorrelation not present). &gt;1.65 = 90% etc…like we saw in the lecture…\n\nbreaks1&lt;-c(-1,-0.5,0,0.5,1)\n\nbreaks2&lt;-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)\n\nNow create a new diverging colour brewer palette and reverse the order using rev() (reverse) so higher values correspond to red - see https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html\n\nMoranColours&lt;- rev(brewer.pal(8, \"RdGy\"))\n\nRemember Moran’s I - test tells us whether we have clustered values (close to 1) or dispersed values (close to -1) of similar values based on the spatial weight matrix that identifies the neighoburs\nBut… the z-score shows were this is unlikely because of complete spatial randomness and so we have spatial clustering (either clustering of similar or dissimilar values depending on the Moran’s I value) within these locations…\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_i\",\n        style=\"fixed\",\n        breaks=breaks1,\n        palette=MoranColours,\n        midpoint=NA,\n        title=\"Local Moran's I\")\n\nWarning: Values have found that are less than the lowest break\n\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_iz\",\n        style=\"fixed\",\n        breaks=breaks2,\n        palette=MoranColours,\n        midpoint=NA,\n        title=\"Local Moran's I Z-score\")\n\n\n\n\n\n\n\n\n\n\n2.6.2 Getis Ord \\(G_{i}^{*}\\)\nWhat about the Getis Ord \\(G_{i}^{*}\\) statistic for hot and cold spots…\nThis is a very similar concept to Local Moran’s I except it just returns a z-score…remember that a z-score shows how many standard deviations a value (our value) is away (above or below) from the mean (of the expected values)\nUltimately a z-score is defined as:\n\\[Z = \\frac{x-\\mu}{\\sigma}\\] Where:\n\n\\(x\\) = the observed value\n\\(\\mu\\) = the mean of the sample\n\\(\\sigma\\) = standard deviation of sample\n\nNote, consult the Global vs location spatial autocorrelation resource for how this is computed in Local Moran’s I if you are interested, although interpretation is the most important part here.\nHowever, in the case of Getis Ord \\(G_{i}^{*}\\) this is the local sum (of the neighbourhood) compared to the sum of all features\nIn Moran’s I this is just the value of the spatial unit (e.g. polygon of the ward) compared to the neighbouring units.\nHere, to be significant (or a hot spot) we will have a high value surrounded by high values. The local sum of these values will be different to the expected sum (think of this as all the values in the area) then where this difference is large we can consider it to be not by chance…\nThe same z-score criteria then applies as before..\nThis summary from L3 Harris nicely summaries the Getis Ord \\(G_{i}^{*}\\) output…\n\nThe result of Getis Ord \\(G_{i}^{*}\\) analysis is an array of Z-scores, one for each pixel [or polygon], which is the number of standard deviations that the pixel [or polygon] and its neighbors are from the global mean. High Z-scores indicate more intense clustering of high pixel values, indicating hot spots. Low Z-scores indicate more intense clustering of low values, indicating cold spots. Individual pixels with high or low values by themselves might be interesting but not necessarily significant.\n\n\ngi_lward_local_density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  localG(., lward_lw)\n\npoints_sf_joined &lt;- points_sf_joined %&gt;%\n  mutate(density_G = as.numeric(gi_lward_local_density))\n\nNote that because of the differences in Moran’s I and Getis Ord \\(G_{i}^{*}\\) there will be differences between polygons that are classed as significant.\nAnd map the outputs…\n\ngicolours&lt;- rev(brewer.pal(8, \"RdBu\"))\n\n#now plot on an interactive map\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_G\",\n        style=\"fixed\",\n        breaks=breaks2,\n        palette=gicolours,\n        midpoint=NA,\n        title=\"Gi* Z score\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#note",
    "href": "02_point_patterns.html#note",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.7 Note",
    "text": "2.7 Note\nIn the analysis you might see a Moran plot where the values of our variable (density of pharmacies) and plotted against (on the y axis) the spatially lagged version (the average value of the same attribute at neighboring locations). However this plot below shows the value of density in relation to the spatial weight matrix…\nThis is useful as we can express the level of spatial association of each observation with its neighboring ones. Points in the upper right (or high-high) and lower left (or low-low) quadrants indicate positive spatial association of values that are higher and lower than the sample mean, respectively. The lower right (or high-low) and upper left (or low-high) quadrants include observations that exhibit negative spatial association; that is, these observed values carry little similarity to their neighboring ones.\nSource: [STAT user guide](https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_variogram_details31.htm#:~:text=The%20Moran%20scatter%20plot%20(Anselin,known%20as%20the%20response%20axis.)\n\nmoran_plot_lward_global_density &lt;- points_sf_joined %&gt;%\n  pull(density)%&gt;%\n  as.vector()%&gt;%\n  moran.plot(., lward_lw)\n\n\n\n\n\n\n\n\nWhen you see Moran’s I out in the wild you will come across maps with:\n\nhigh values surrounded by high values (HH)\nlow values nearby other low values (LL)\nlow values among high values (LH)\nhigh values among low values (HL)\n\nHere, we use the values we have, of density and Moran’s I, compared to the mean of density and Moran’s I (termed centering). Where the:\n\nvalue of density is greater than 0 and the value of Moran’s I is greater than 0 then high values (of density) are surrounded by other high values (from Moran’s I)= HH\nvalue of density is lower than 0 and the value of Moran’s I is lower than 0 then low values (of density) are surrounded by other low values (from Moran’s I) = LL\nvalue of density is lower than 0 and the value of Moran’s I is higher than 0 then low values (of density) are surrounded by high values (from Moran’s I) = LH\nvalue of density is higher than 0 and the value of Moran’s I is lower than 0 then high values (of density) are surrounded by high values (from Moran’s I) =HL\n\n\nsignif &lt;- 0.1\n\n\n# centers the variable of interest around its mean\npoints_sf_joined2 &lt;- points_sf_joined %&gt;%\n  mutate(mean_density = density- mean(density))%&gt;%\n  mutate(mean_density = as.vector(mean_density))%&gt;%\n  mutate(mean_densityI= density_i - mean(density_i))%&gt;%\n  mutate(quadrant = case_when(mean_density&gt;0 & mean_densityI &gt;0 ~ 4,\n         mean_density&lt;0 & mean_densityI &lt;0 ~ 1,\n         mean_density&lt;0 & mean_densityI &gt;0 ~ 2,\n         mean_density&gt;0 & mean_densityI &lt;0 ~ 3))%&gt;%\n  mutate(quadrant=case_when(p &gt; signif ~ 0, TRUE ~ quadrant))\n\nbrks &lt;- c(0,1,2,3,4,5)\ncolors &lt;- c(\"white\",\"blue\",\"skyblue\",\"pink\",\"red\")\n\n\ntm_shape(points_sf_joined2) +\n    tm_polygons(\"quadrant\",\n        style=\"fixed\",\n        breaks=brks,\n        labels = c(\"insignificant\",\"low-low\",\"low-high\",\"high-low\",\"high-high\"),\n        palette=colors,\n        title=\"Moran's I HH etc\")\n\n\n\n\n\n\n\n\nSource: Carlos Mendez\nThis might seem somewhat confusing as if we look in the South East we have low values of Getis Ord \\(G_{i}^{*}\\) yet we have shown that these are low (density) and high Moran’s I. But as Matthew Peeples concisely summarises remember…\n\nMoran’s I is a measure of the degree to which the value at a target site is similar to values at adjacent sites. Moran’s I is large and positive when the value for a given target (or for all locations in the global case) is similar to adjacent values and negative when the value at a target is dissimilar to adjacent values.\nGetis Ord \\(G_{i}^{*}\\) identifies areas where high or low values cluster in space. It is high where the sum of values within a neighborhood of a given radius or configuration is high relative to the global average and negative where the sum of values within a neighborhood are small relative to the global average and approaches 0 at intermediate values.\n\nSo here we have a high Moran’s I as the values around it are similar (probably all low) but a low Getis Ord as the values within the local area are low relative to the global average.\n\n2.7.1 Consider what this all means\n\nDo you think Pharmacies take into account the ward they are in when they open?\nDo you think people only go to pharmacies within their ward?\nPharmacies don’t exhibit complete spatial randomness and there seems to be clustering in certain locations….\nAnother question we could move onto is now are pharmacies locating around a need (e.g. health outcomes) and does that mean some of the population in London have poor access or choice.\nFor example….we could now look to see if there is clustering of “Deaths from causes considered preventable, under 75 years, standardised mortality ratio” and if that aligns with (or can be explained by) access / clusters of pharmacies.\nIf you wanted to do this (e.g. see if the ratio was clustered), the data is in a somewhat unfriendly format so you’d need to:\n\nDownload the public health profile ward data:\nRead the Indicator definitions for the last row, specifically column G which tells us how it’s created.\nFilter the data based on the indicator value of 93480 (from the Indicator definitions)\nFilter out the London wards - as we did here and join it to our spatial feature.\nRun a Moran’s I or some other spatial autocorrelation\nJoin this back to our spatial feature that has the Moran’s I for pharmacy density\nDecide what to plot - does dispersion of pharmacies occur in the same spatial units as clustering of deaths considered preventable? This could simply be two maps and a table.\n\nShould it be a requirement to have access…well if we consult the “2022 Pharmacy Access Scheme: guidance” then yes it appears so but…“Pharmacies in areas with dense provision of pharmacies remain excluded from the scheme”\n\n\n\n2.7.2 Extensions\n\nThe use of OPTICS\nWe have spatial autocorrelation now can we try and model local differences in density of pharmacies - i.e. what factors might (or might not) explain the difference here\nOr perhaps does the distance to pharmacies assist in explaining deaths from causes considered preventable, under 75 years. Similarly, we could even use this as a dummy variable (yes/no the areas is / is not in a cluster of pharmacies) in a regression model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "03_obesity.html",
    "href": "03_obesity.html",
    "title": "3  Obesity",
    "section": "",
    "text": "AD / CC to add",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Obesity</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html",
    "href": "04_spatial_models.html",
    "title": "4  Spatial models",
    "section": "",
    "text": "4.1 Introduction\nIn this practical you will be introduced to a suite of different models that will allow you to test a variety of research questions and hypotheses through modelling the associations between two or more spatially referenced variables.\nIn 2023 New Zealand scrapped an anti-smoking law that would mean a generational smoking bad for anyone born after 2008. So, here we will explore the factors that might affect smoking in New Zealand and see if they vary spatially.\nThis practical will walk you through the common steps that you should go through when building a regression model using spatial data to test a stated research hypothesis; from carrying out some descriptive visualisation and summary statistics, to interpreting the results and using the outputs of the model to inform your next steps.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#data",
    "href": "04_spatial_models.html#data",
    "title": "4  Spatial models",
    "section": "4.2 Data",
    "text": "4.2 Data\nThis is the same has homework 1….\n\nGo to the New Zealand spatial data portal and download the file Territorial Authority 2018 (generalised), these are city or district councils. Make sure it’s the Territorial Authority 2018 data not SA1.\nGo to the Stats NZ website and download the Statistical area 1 dataset for 2018 for the whole of New Zealand. Download the csv this time\n\nThe figure below explains the geographies of New Zealand, we could replicate this analysis for SA1 or SA2 data.\n\n\n\n\n\n\n\nNZ geographies\n\n\n\n4.2.1 Loading\n\nlibrary(sf)\nlibrary(tidyverse)\n\nta &lt;- st_read(\"prac4_data/statsnzterritorial-authority-2018-generalised-SHP/territorial-authority-2018-generalised.shp\")\n\nReading layer `territorial-authority-2018-generalised' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_25\\spatial analysis of public health\\spatial-analysis-of-public-health-data-25\\prac4_data\\statsnzterritorial-authority-2018-generalised-SHP\\territorial-authority-2018-generalised.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 68 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1067061 ymin: 4701317 xmax: 2523320 ymax: 6242140\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\ncensus &lt;- read_csv(\"prac4_data/SA1_census/Individual_part2_totalNZ-wide_format_updated_16-7-20.csv\")\n\n\n\n4.2.2 Wrangle\nThe .csv we have loaded contains all the data from the week 1 excel homework in a wide format - that means all the SA1s, SA2s etc are are a new row making a huge data file. Normally i would suggest a cool data sci-ency way to pull out our Territorial Authorities, however i can’t see anything that would work. For example, we could try and detect part of a string like “City” or “District” but they appear in other geographies.\nSo, until i can think or a better way we are resigned to subsetting by rows!\n\nlibrary(janitor)\n\ncensus_authorities &lt;- census %&gt;%\n  slice(32413:32480) %&gt;%\n  clean_names()\n\n# or\n\n#census_authorities &lt;- census[32413:32480,]%&gt;%\n#  clean_names()\n\nNow in this case the data can be a little hard to read - we have over 350 columns! It might be sensible here to have a look at the excel document we downloaded in the week 1 homework to identify our our variables.\nLet’s pull out the following variables from the 2018 data:\n\nregular smoker\n\nthat will be modeled by:\n\nmedian income\nhome ownership\nno qualification, see the New Zealand gov careers website for what qualification levels mean\nwe’ll also try and include some spatial data (e.g. density of tobacco shops) at the very end…\n\nBecause we are using spatial units and they may differ in size we should normalise our count data (everything but median income which is continous).\nTo do so i will use the “total” column for each variable, although you could also use population count\n\ncensus_authorities_data &lt;- census_authorities %&gt;%\n  select(area_code,\n         area_description,\n         census_2018_smoking_status_01_regular_smoker_curp_15years_and_over,\n         census_2018_smoking_status_total_stated_curp_15years_and_over,\n         census_2018_total_personal_income_median_curp_15years_and_over,\n         census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over,\n         census_2018_individual_home_ownership_total_stated_curp_15years_and_over,\n         census_2018_highest_qualification_000_no_qualification_curp_15years_and_over,\n         census_2018_highest_qualification_total_stated_curp_15years_and_over\n         )\n\nLet’s check the type of our data…\n\ndatatypelist &lt;- census_authorities_data %&gt;% \n  summarise_all(class) %&gt;%\n  pivot_longer(everything(), \n               names_to=\"All_variables\", \n               values_to=\"Variable_class\")\n\ndatatypelist\n\n# A tibble: 9 × 2\n  All_variables                                                   Variable_class\n  &lt;chr&gt;                                                           &lt;chr&gt;         \n1 area_code                                                       character     \n2 area_description                                                character     \n3 census_2018_smoking_status_01_regular_smoker_curp_15years_and_… character     \n4 census_2018_smoking_status_total_stated_curp_15years_and_over   character     \n5 census_2018_total_personal_income_median_curp_15years_and_over  character     \n6 census_2018_individual_home_ownership_02_own_or_partly_own_cur… character     \n7 census_2018_individual_home_ownership_total_stated_curp_15year… character     \n8 census_2018_highest_qualification_000_no_qualification_curp_15… character     \n9 census_2018_highest_qualification_total_stated_curp_15years_an… character     \n\n\nCharacter! we will need to change that…\n\ncensus_authorities_data &lt;- census_authorities_data %&gt;%\n  mutate_at(c(\"census_2018_smoking_status_01_regular_smoker_curp_15years_and_over\",\n        \"census_2018_smoking_status_total_stated_curp_15years_and_over\",\n         \"census_2018_total_personal_income_median_curp_15years_and_over\",\n         \"census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over\",\n         \"census_2018_individual_home_ownership_total_stated_curp_15years_and_over\",\n         \"census_2018_highest_qualification_000_no_qualification_curp_15years_and_over\",\n         \"census_2018_highest_qualification_total_stated_curp_15years_and_over\"),\n        as.numeric)\n\nThe data is selected and now numeric… let’s normalise…and select what we need\n\ncensus_authorities_data_norm &lt;- census_authorities_data%&gt;%\n  mutate(percent_regular_smoker=\n           (census_2018_smoking_status_01_regular_smoker_curp_15years_and_over/census_2018_smoking_status_total_stated_curp_15years_and_over)*100)%&gt;%\n  mutate(percent_home_owner=\n           (census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over/census_2018_individual_home_ownership_total_stated_curp_15years_and_over)*100)%&gt;%\n  mutate(percent_no_qualification=\n           (census_2018_highest_qualification_000_no_qualification_curp_15years_and_over/ census_2018_highest_qualification_total_stated_curp_15years_and_over)*100)%&gt;%\n  select(area_code,\n         area_description,\n         percent_regular_smoker,\n         percent_home_owner,\n         percent_no_qualification,\n         census_2018_total_personal_income_median_curp_15years_and_over\n         )\n\nQuick plot…\n\nlibrary(ggplot2)\n\nq &lt;- qplot(x = percent_no_qualification, \n           y = percent_regular_smoker, \n           data=census_authorities_data_norm)\n\nq + stat_smooth(method=\"lm\", se=FALSE, size=1) \n\n\n\n\n\n\n\n\nHere, I have plotted the percentage of regular smokers per territorial area against another variable in the dataset that I think might be influential…percentage with no qualification\nRemember that my null hypothesis would be that there is no relationship between percentage of regular smokers and percentage with no qualification. If this null hypothesis was true, then I would not expect to see any pattern in the cloud of points plotted above.\nAs it is, the scatter plot shows that, generally, as the \\(x\\) axis independent variable (percentage with no qualification) goes up, our \\(y\\) axis dependent variable (percent of smokers) goes up. This is not a random cloud of points, but something that indicates there could be a relationship here and so I might be looking to reject my null hypothesis.\nSome conventions - In a regression equation, the dependent variable is always labelled \\(y\\) and shown on the \\(y\\) axis of a graph, the predictor or independent variable(s) is(are) always shown on the \\(x\\) axis.\nI have added a blue line of best-fit - this is the line that can be drawn by minimising the sum of the squared differences between the line and the residuals. The residuals are all of the dots not falling exactly on the blue line. An algorithm known as ‘ordinary least squares’ (OLS) is used to draw this line and it simply tries a selection of different lines until the sum of the squared differences between all of the residuals and the blue line is minimised, leaving the final solution.\nAs a general rule, the better the blue line is at summarising the relationship between \\(y\\) and \\(x\\), the better the model.\nThe equation for the blue line in the graph above can be written:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\]\nwhere:\n\\(\\beta_0\\) is the intercept (the value of \\(y\\) when \\(x = 0\\) - somewhere around 4 on the graph above);\n\\(\\beta_1\\) is sometimes referred to as the ‘slope’ parameter and is simply the change in the value of \\(y\\) for a 1 unit change in the value of \\(x\\) (the slope of the blue line) - reading the graph above, the change in the value of \\(y\\) reading between 20 and 15 on the \\(x\\) axis looks to be around a change from 15 to 17.5 on the \\(y\\) - about 0.5 per 1 unit value of \\(x\\).\n\\(\\epsilon_i\\) is a random error term (positive or negative) that should sum to 0 - essentially, if you add all of the vertical differences between the blue line and all of the residuals, it should sum to 0.\nAny value of \\(y\\) along the blue line can be modeled using the corresponding value of \\(x\\) and these parameter values. Examining the graph above we would expect the percent smoking in an area where 26% of the population to not have any qualifications, to equal around 17%, but we can confirm this by plugging the \\(\\beta\\) parameter values and the value of \\(x\\) into equation (1):\n\n4 + (0.5*26) + 0\n\n[1] 17",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#regression",
    "href": "04_spatial_models.html#regression",
    "title": "4  Spatial models",
    "section": "4.3 Regression",
    "text": "4.3 Regression\nIn the graph above, I used a method called ‘lm’ in the stat_smooth() function in ggplot2 to draw the regression line. ‘lm’ stands for ‘linear model’ and is a standard function in R for running linear regression models. Use the help system to find out more about lm - ?lm\nBelow is the code that could be used to draw the blue line in our scatter plot. Note, the tilde ~ symbol means “is modeled by”.\n\n#now model\nmodel1 &lt;- census_authorities_data_norm %&gt;%\n  lm(percent_regular_smoker ~\n       percent_no_qualification,\n     data=.)\n\nLet’s have a closer look at our model…\n\n#show the summary of those outputs\nsummary(model1)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification, \n    data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7961 -2.3344 -0.5826  1.1691 16.4704 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               1.20258    1.98667   0.605    0.547    \npercent_no_qualification  0.64634    0.08289   7.798  6.1e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.588 on 66 degrees of freedom\nMultiple R-squared:  0.4795,    Adjusted R-squared:  0.4716 \nF-statistic:  60.8 on 1 and 66 DF,  p-value: 6.097e-11\n\n\n\n4.3.1 Interpretation\nIn running a regression model, we are effectively trying to test (disprove) our null hypothesis. If our null hypothesis was true, then we would expect our coefficients to = 0.\nIn the output summary of the model above, there are a number of features you should pay attention to:\nCoefficient Estimates - these are the \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope) parameter estimates from Equation 1. You will notice that at \\(\\beta_0 = 1.2\\) and \\(\\beta_1 = 0.64\\) they are pretty close (not awful!) to the estimates of 4 and 0.5 that we read from the graph earlier, but more precise.\nCoefficient Standard Errors - these represent the average amount the coefficient varies from the average value of the dependent variable (its standard deviation). So, for a 1% increase in percent with no qualifications, while the model says we might expect percent of smokers to rise by 0.6%, this might vary, on average, by about 0.08%. As a rule of thumb, we are looking for a lower value in the standard error relative to the size of the coefficient.\nNote that is the coefficient represents a one unit change, here it is %, as the variable is % unauthorized absence in school So one unit is a 1% change…\nCoefficient t-value - this is the value of the coefficient divided by the standard error and so can be thought of as a kind of standardised coefficient value. The larger (either positive or negative) the value the greater the relative effect that particular independent variable is having on the dependent variable (this is perhaps more useful when we have several independent variables in the model) .\nCoefficient p-value - Pr(&gt;|t|) - the p-value is a measure of significance. There is lots of debate about p-values which I won’t go into here, but essentially it refers to the probability of getting a coefficient as large as the one observed in a set of random data. p-values can be thought of as percentages, so if we have a p-value of 0.5, then there is a 5% chance that our coefficient could have occurred in some random data, or put another way, a 95% chance that out coefficient could have only occurred in our data.\nAs a rule of thumb, the smaller the p-value, the more significant that variable is in the story and the smaller the chance that the relationship being observed is just random. Generally, statisticians use 5% or 0.05 as the acceptable cut-off for statistical significance - anything greater than that we should be a little sceptical about.\nIn r the codes ***, **, **, . are used to indicate significance. We generally want at least a single * next to our coefficient for it to be worth considering.\nR-Squared - This can be thought of as an indication of how good your model is - a measure of ‘goodness-of-fit’ (of which there are a number of others). \\(r^2\\) is quite an intuitive measure of fit as it ranges between 0 and 1 and can be thought of as the % of variation in the dependent variable (in our case percentage of smokers) explained by variation in the independent variable(s). In our example, an \\(r^2\\) value of 0.47 indicates that around 47% of the variation in percentage of smokers can be explained by variation in percentage with no qualifications. In other words, this is quite a good model. The \\(r^2\\) value will increase as more independent explanatory variables are added into the model, so where this might be an issue, the adjusted r-squared value can be used to account for this affect.\n\n\n4.3.2 Assumptions underpinning linear regression\n\n4.3.2.1 Assumption 1 - there is a linear relationship between the dependent and independent variables\nThe best way to test for this assumption is to plot a scatter plot similar to the one created earlier. It may not always be practical to create a series of scatter plots, so one quick way to check that a linear relationship is probable is to look at the frequency distributions of the variables. If they are normally distributed, then there is a good chance that if the two variables are in some way correlated, this will be a linear relationship.\nFor example, look at the frequency distributions of our two variables earlier:\n\n#let's check the distribution of these variables first\n\nggplot(census_authorities_data_norm,\n       aes(x=percent_regular_smoker)) + \n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 1) + \n  geom_density(colour=\"red\", \n               size=1, \n               adjust=1)\n\n\n\n\n\n\n\n\nHere, adding after_stat(density) means that the histogram is a density plot, this plots the chance that any value in the data is equal to that value.\n\nggplot(census_authorities_data_norm,\n       aes(x=percent_home_owner)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 1) + \n  geom_density(colour=\"red\",\n               size=1, \n               adjust=1)\n\n\n\n\n\n\n\n\nIt is not a requirement of regression to have normally distributed variables, however if they aren’t the residuals may well be normally distributed, but could vary (meaning we have hetroscedasticity) which we discuss later on.\nOne way that we might be able to achieve a linear relationship between our two variables is to transform the non-normally distributed variable so that it is more normally distributed. For more information on this see transforming variables.\nHowever you will be changing the relationship of your data - it won’t be linear anymore! This could improve your model but is at the expense of interpretation. Aside from log transformation which has the rules in the link above.\nTypically if you do a power transformation you can keep the direction of the relationship (positive or negative) and the t-value will give an idea of the importance of the variable in the model - that’s about it!\n\n\n4.3.2.2 Assumption 2 - the residuals in your model should be normally distributed\nThis assumption is easy to check. When we ran our model1 earlier, one of the outputs stored in our model1 object is the residual value for each case (authority) in your dataset. We can access these values using augment() from broom which will add model output to the original data…\nWe can plot these as a histogram and see if there is a normal distribution:\n\nlibrary(broom)\n\n#save the residuals into your dataframe\nmodel_data &lt;- model1 %&gt;%\n  augment(., census_authorities_data_norm)\n\n#plot residuals\nmodel_data%&gt;%\ndplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  qplot()+ \n  geom_histogram() \n\n\n\n\n\n\n\n\nExamining the histogram above, i am happy this is rather normal although there is evidently an outlier in our data somewhere.\n\n\n4.3.2.3 Assumption 3 - no multicolinearity in the independent variables\nNow, the regression model we have be experimenting with so far is a simple bivariate (two variable) model. One of the nice things about regression modelling is while we can only easily visualise linear relationships in a two (or maximum 3) dimension scatter plot, mathematically, we can have as many dimensions / variables as we like.\nAs such, we could extend model 1 into a multiple regression model by adding some more explanatory variables that we think could affect the percentage of people smoking…\n\nmodel2 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n             data = census_authorities_data_norm)\n\nsummary(model2)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = census_authorities_data_norm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9633 -1.6181 -0.4151  1.3204 12.1627 \n\nCoefficients:\n                                                                 Estimate\n(Intercept)                                                     1.762e+01\npercent_no_qualification                                        7.757e-01\npercent_home_owner                                             -3.992e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -3.922e-05\n                                                               Std. Error\n(Intercept)                                                     5.089e+00\npercent_no_qualification                                        9.198e-02\npercent_home_owner                                              7.015e-02\ncensus_2018_total_personal_income_median_curp_15years_and_over  9.961e-05\n                                                               t value Pr(&gt;|t|)\n(Intercept)                                                      3.463 0.000959\npercent_no_qualification                                         8.433 5.57e-12\npercent_home_owner                                              -5.690 3.40e-07\ncensus_2018_total_personal_income_median_curp_15years_and_over  -0.394 0.695119\n                                                                  \n(Intercept)                                                    ***\npercent_no_qualification                                       ***\npercent_home_owner                                             ***\ncensus_2018_total_personal_income_median_curp_15years_and_over    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.96 on 64 degrees of freedom\nMultiple R-squared:  0.6564,    Adjusted R-squared:  0.6403 \nF-statistic: 40.75 on 3 and 64 DF,  p-value: 7.514e-15\n\n\nExamining the output above, it is clear that including these variables into our model improves the fit from an \\(r^2\\) of around 47% to an \\(r^2\\) of 65%. However, income is not significant so we should consider removing it..\n\nmodel2 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner ,\n             data = census_authorities_data_norm)\n\nsummary(model2)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner, data = census_authorities_data_norm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6020 -1.6175 -0.3927  1.2883 11.7795 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              16.02096    3.04321   5.264 1.70e-06 ***\npercent_no_qualification  0.79755    0.07283  10.950  &lt; 2e-16 ***\npercent_home_owner       -0.40092    0.06955  -5.764 2.45e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 65 degrees of freedom\nMultiple R-squared:  0.6556,    Adjusted R-squared:  0.645 \nF-statistic: 61.86 on 2 and 65 DF,  p-value: 9.037e-16\n\n\nHere we see the \\(r^2\\) is almost the same. But do our two explanatory variables satisfy the no-multicoliniarity assumption? If not and the variables are highly correlated, then we are effectively double counting the influence of these variables and overstating their explanatory power\n\n\n\n\n\nWord anatomy of multicollinearity. Source: What the Heck is Multicollinearity?, Andrew Ozbun\n\n\n\n\nTo check this, we can compute the product moment correlation coefficient between the variables, using the corrr() package, that’s part of tidymodels. In an ideal world, we would be looking for something less than a 0.8 correlation\n\nlibrary(corrr)\n\ncorrelation &lt;- census_authorities_data_norm %&gt;%\n  dplyr::select(percent_no_qualification, \n               percent_home_owner)%&gt;%\n    correlate()\n\n#visualise the correlation matrix\nrplot(correlation)\n\n\n\n\n\n\n\ncorrelation\n\n# A tibble: 2 × 3\n  term                     percent_no_qualification percent_home_owner\n  &lt;chr&gt;                                       &lt;dbl&gt;              &lt;dbl&gt;\n1 percent_no_qualification                   NA                  0.360\n2 percent_home_owner                          0.360             NA    \n\n\nAnother way that we can check for Multicolinearity is to examine the VIF for the model. If we have VIF values for any variable exceeding 10, then we may need to worry and perhaps remove that variable from the analysis:\n\nlibrary(car)\nvif(model2)\n\npercent_no_qualification       percent_home_owner \n                1.149067                 1.149067 \n\n\n\n\n4.3.2.4 Assumption 4 - homoscedasticity\nHomoscedasticity means that the errors/residuals in the model exhibit constant / homogeneous variance, if they don’t, then we would say that there is hetroscedasticity present. Why does this matter? Andy Field does a much better job of explaining this in discovering statistics — but essentially, if your errors do not have constant variance, then your parameter estimates could be wrong, as could the estimates of their significance.\nThe best way to check for homo/hetroscedasticity is to plot the residuals in the model against the predicted values. We are looking for a cloud of points with no apparent patterning to them.\n\n#print some model diagnositcs. \npar(mfrow=c(2,2))    #plot to 2 by 2 array\nplot(model2)\n\n\n\n\n\n\n\n\nIn the series of plots above, the first plot (residuals vs fitted), we would hope to find a random cloud of points with a straight horizontal red line. Looking at the plot, the curved red line would suggest some hetroscedasticity, but the cloud looks quite random. Similarly we are looking for a random cloud of points with no apparent patterning or shape in the third plot of standardised residuals vs fitted values. Here, the cloud of points also looks fairly random, with perhaps some shaping indicated by the red line.\nSection 5.7.6 in Tidyverse Skills for Data Science explains each of these plots in more detail\nIn the plots here we are looking for:\n\nResiduals vs Fitted: a flat and horizontal line. This is looking at the linear relationship assumption between our variables\nNormal Q-Q: all points falling on the line. This checks if the residuals (observed minus predicted) are normally distributed\nScale vs Location: flat and horizontal line, with randomly spaced points. This is the homoscedasticity (errors/residuals in the model exhibit constant / homogeneous variance). Are the residuals (also called errors) spread equally along all of the data.\nResiduals vs Leverage - Identifies outliers (or influential observations), the three largest outliers are identified with values in the plot.\n\nThe University of Viginia Library provides examples of good and bad models in relation to these plots.\nThere is an easier way to produce this plot using check_model() from the performance package, that even includes what you are looking for…note that the Posterior predictive check is the comparison between the fitted model and the observed data.\nThe default argument is check=all but we can specify what to check for…see the arguments section in the documentation…e.g. check = c(\"vif\", \"qq\")\n\nlibrary(performance)\n\ncheck_model(model2, check=\"all\")\n\n\n\n\n\n\n\n\n\n\n4.3.2.5 Assumption 5 - independence of errors\nThis assumption simply states that the residual values (errors) in your model must not be correlated in any way. If they are, then they exhibit autocorrelation which suggests that something might be going on in the background that we have not sufficiently accounted for in our model.\n\n4.3.2.5.1 Standard autocorrelation\nIf you are running a regression model on data that do not have explicit space or time dimensions, then the standard test for autocorrelation would be the Durbin-Watson test.\nThis tests whether residuals are correlated and produces a summary statistic that ranges between 0 and 4, with 2 signifying no autocorrelation. A value greater than 2 suggesting negative autocorrelation and and value of less than 2 indicating positive autocorrelation.\nIn his excellent text book, Andy Field suggests that you should be concerned with Durbin-Watson test statistics &lt;1 or &gt;3. So let’s see:\n\n#run durbin-watson test\ndw &lt;- durbinWatsonTest(model2)\ntidy(dw)\n\n# A tibble: 1 × 5\n  statistic p.value autocorrelation method             alternative\n      &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1      1.41 0.00600           0.227 Durbin-Watson Test two.sided  \n\n\nAs you can see, the DW statistics for our model is 1.41, so some indication of autocorrelation, but perhaps nothing to worry about.\n\n\n4.3.2.5.2 Spatial autocorrelation\nHOWEVER\nWe are using spatially referenced data and as such we should check for spatial-autocorrelation.\nThe first test we should carry out is to map the residuals to see if there are any apparent obvious patterns:\n\n#and for future use, write the residuals out\nmodel2_residuals &lt;- model2 %&gt;%\n  augment(., census_authorities_data_norm)\n\n\nta_model_data&lt;- ta%&gt;%\n  clean_names(.)%&gt;%\n  left_join(., model2_residuals,\n            by=c(\"ta2018_v1\"=\"area_code\"))\n\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n#now plot the residuals\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(ta_model_data) +\n  tm_polygons(\".resid\",\n              palette = \"RdYlBu\")\n\nVariable(s) \".resid\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nNow, we can immediately see some issues with this…\n\nI have left the data “outside territorial authority” in the data and it has a large negative residual\nChatham Islands has been included and is also a large positive residual.\n\nFor the purposes of this practical let’s remove them and re-run the model…\nNote i have added in median income and it is now significant!\n\ncensus_authorities_data_norm_filter &lt;- census_authorities_data_norm %&gt;%\n  # != means not equal to\n  filter(area_code != \"999\" & area_code != \"067\")\n\nmodel3 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n             data = census_authorities_data_norm_filter)\n\nsummary(model3)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = census_authorities_data_norm_filter)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6984 -1.5153 -0.4655  1.2802  5.3085 \n\nCoefficients:\n                                                                 Estimate\n(Intercept)                                                     2.796e+01\npercent_no_qualification                                        5.152e-01\npercent_home_owner                                             -3.051e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -3.295e-04\n                                                               Std. Error\n(Intercept)                                                     3.939e+00\npercent_no_qualification                                        7.357e-02\npercent_home_owner                                              5.305e-02\ncensus_2018_total_personal_income_median_curp_15years_and_over  8.034e-05\n                                                               t value Pr(&gt;|t|)\n(Intercept)                                                      7.098 1.45e-09\npercent_no_qualification                                         7.002 2.13e-09\npercent_home_owner                                              -5.752 2.92e-07\ncensus_2018_total_personal_income_median_curp_15years_and_over  -4.101 0.000122\n                                                                  \n(Intercept)                                                    ***\npercent_no_qualification                                       ***\npercent_home_owner                                             ***\ncensus_2018_total_personal_income_median_curp_15years_and_over ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.118 on 62 degrees of freedom\nMultiple R-squared:  0.7607,    Adjusted R-squared:  0.7491 \nF-statistic: 65.68 on 3 and 62 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that the \\(r^2\\) has jumped to 76%…now Durbin Watson test and map again.\n\ndw &lt;- durbinWatsonTest(model3)\ntidy(dw)\n\n# A tibble: 1 × 5\n  statistic p.value autocorrelation method             alternative\n      &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1      1.27 0.00200           0.345 Durbin-Watson Test two.sided  \n\n#and for future use, write the residuals out\nmodel3_residuals &lt;- model3 %&gt;%\n  augment(., census_authorities_data_norm_filter)\n\n\nta_model3_data&lt;- ta%&gt;%\n  clean_names(.)%&gt;%\n  left_join(., model3_residuals,\n            by=c(\"ta2018_v1\"=\"area_code\"))\n\ntm_shape(ta_model3_data) +\n  tm_polygons(\".resid\",\n              palette = \"RdYlBu\")\n\n\n\n\n\n\n\n\nThis suggests that there could well be some spatial autocorrelation biasing our model, but can we test for spatial autocorrelation more systematically?\nYes - and some of you will remember this from the practical a few days ago. We can calculate a number of different statistics to check for spatial autocorrelation - the most common of these being Moran’s I.\n\n#calculate the centroids \n\nta_model3_data &lt;- ta_model3_data %&gt;%\n  filter(ta2018_v1 != \"999\" & ta2018_v1 != \"067\")\n\ncoordsW &lt;- ta_model3_data%&gt;%\n  st_centroid()%&gt;%\n  st_geometry()\n\nplot(coordsW)\n\n\n\n\n\n\n\n\n\nlibrary(spdep)\n\n#Now we need to generate a spatial weights matrix \n#We'll start with a simple binary matrix of queen's case neighbours\n\nta_nb &lt;- ta_model3_data %&gt;%\n  poly2nb(., queen=T)\n\n#or nearest neighbours\nta_knn &lt;-coordsW %&gt;%\n  knearneigh(., k=4)\n\nta_knn_nb &lt;- ta_knn %&gt;%\n  knn2nb()\n\n#plot them\nplot(ta_nb, st_geometry(coordsW), col=\"red\")\n\n\n\n\n\n\n\n\n\n#create a spatial weights matrix object from these weights\n\nta_queens_weight &lt;- ta_nb %&gt;%\n  nb2listw(., style=\"W\")\n\nta_knn_4_weight &lt;- ta_knn_nb %&gt;%\n  nb2listw(., style=\"W\")\n\nThe style argument means the style of the output — B is binary encoding listing them as neighbours or not, W row standardised that we saw yesterday.\nNow run a Moran’s I test on the residuals, first using queens neighbours\n\nqueen &lt;- ta_model3_data %&gt;%\n  st_drop_geometry()%&gt;%\n  dplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  moran.test(., ta_queens_weight)%&gt;%\n  tidy()\n\nThen nearest k-nearest neighbours\n\nnearest_neighbour &lt;- ta_model3_data %&gt;%\n  st_drop_geometry()%&gt;%\n  dplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  moran.test(., ta_knn_4_weight)%&gt;%\n  tidy()\n\nqueen\n\n# A tibble: 1 × 7\n  estimate1 estimate2 estimate3 statistic   p.value method           alternative\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;      \n1     0.342   -0.0154   0.00818      3.95 0.0000387 Moran I test un… greater    \n\nnearest_neighbour\n\n# A tibble: 1 × 7\n  estimate1 estimate2 estimate3 statistic    p.value method          alternative\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;      \n1     0.334   -0.0154   0.00622      4.43 0.00000474 Moran I test u… greater    \n\n\nObserving the Moran’s I statistic for both Queen’s case neighbours and k-nearest neighbours of 4, we can see that the Moran’s I statistic is somewhere between 0.34 and 0.33. Remembering that Moran’s I ranges from between -1 and +1 (0 indicating no spatial autocorrelation) we can conclude that there is some weak to moderate spatial autocorrelation in our residuals.\nThis means that despite passing most of the assumptions of linear regression, we could have a situation here where the presence of some spatial autocorrelation could be leading to biased estimates of our parameters and significance values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#spatial-regression-models",
    "href": "04_spatial_models.html#spatial-regression-models",
    "title": "4  Spatial models",
    "section": "4.4 Spatial regression models",
    "text": "4.4 Spatial regression models\n\n4.4.1 The spatial lag (lagged dependent variable) model\nRunning a Moran’s I test on the residuals from the model suggested that there might be some spatial autocorrelation occurring suggesting that places where the model over-predicted the percent of people smoking (those shown in blue in the map above with negative residuals) and under-predicted (those shown in red/orange) occasionally were near to each other.\nWard and Gleditsch (2008) describe this situation (where the value of our \\(y\\) dependent variable - percent people who smoke - may be influenced by neighbouring values) and suggest the way to deal with it is to incorporate a spatially-lagged version of this variable amongst the independent variables on the right-hand side of the equation. In this case, Equation 1 would be updated to look like this:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\rho w_i.y_i + \\epsilon_i\\]\nIn this equation, \\(w\\) is the spatial weights matrix you generated and \\(w_i\\) is vector of all neighbouring areas (in our case, Territorial Authority) for any Territorial Authority \\(y_i\\). If using binary this will be a sum (e.g. the value of \\(y\\) plotted against the sum of \\(y\\) of the neighbouring Territorial Authority). This may be explained as a weighted sum as the non-neighbours will be removed, the weight being the neighbours If this is row standardised it will be a weighted average of the neighbouring \\(y\\) values. Typically we use a row standardised matrix.\nIn this model, a positive value for the \\(\\rho w_i.y_i\\) parameter would indicate that the average value for the percent smoking is expected to be higher if, on average, neighbouring authorities also have a higher percentage of people who smoke.\n\n\\(\\rho\\) denotes (represents) the spatial lag\n\nFor more details on running a spatially lagged regression model and interpreting the outputs, see the chapter on spatially lagged models by Ward and Gleditsch (2008) available online\n\n4.4.1.1 Queen’s case lag\nNow run a spatially-lagged regression model with a queen’s case weights matrix\n\nlibrary(spatialreg)\n\nslag_dv_model2_queen &lt;- lagsarlm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data, \n               nb2listw(ta_nb, style=\"C\"), \n               method = \"eigen\")\n\n#what do the outputs show?\ntidy(slag_dv_model2_queen)\n\n# A tibble: 5 × 5\n  term                                     estimate std.error statistic  p.value\n  &lt;chr&gt;                                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 rho                                       2.33e-2 0.0311        0.748 4.55e- 1\n2 (Intercept)                               2.76e+1 3.82          7.23  4.66e-13\n3 percent_no_qualification                  5.08e-1 0.0720        7.06  1.67e-12\n4 percent_home_owner                       -3.00e-1 0.0516       -5.82  5.99e- 9\n5 census_2018_total_personal_income_media… -3.33e-4 0.0000777    -4.28  1.85e- 5\n\n#glance() gives model stats but this need something produced from a linear model\n#here we have used lagsarlm()\nglance(slag_dv_model2_queen)\n\n# A tibble: 1 × 6\n  r.squared   AIC   BIC deviance logLik  nobs\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1     0.763  294.  307.     276.  -141.    66\n\nhold&lt;-summary(slag_dv_model2_queen)\n\nsum(hold$residuals)\n\n[1] -3.039236e-15\n\n\nRunning the spatially-lagged model with a Queen’s case spatial weights matrix reveals that in this example, there is an insignificant and small effect associated with the spatially lagged dependent variable. However, a different conception of neighbours we might get a different outcome….\nHere:\n\nRho is our spatial lag (0.0232) that measures the variable in the surrounding spatial areas as defined by the spatial weight matrix. We use this as an extra explanatory variable to account for clustering (identified by Moran’s I). If significant it means that the percent of smokers in a unit vary based on the percent of smokers in the neighboring units. If it is positive it means as the percent of smokers increase in the surrounding units so does our central value\nLog likelihood shows how well the data fits the model (like the AIC, which we cover later), the higher the value the better the models fits the data.\nLikelihood ratio (LR) test shows if the addition of the lag is an improvement (from linear regression) and if that’s significant. This code would give the same output…\n\n\nlibrary(lmtest)\nlrtest(slag_dv_model2_queen, model3)\n\n\nLagrange Multiplier (LM) is a test for the absence of spatial autocorrelation in the lag model residuals. If significant then you can reject the Null (no spatial autocorrelation) and accept the alternative (is spatial autocorrelcation)\nWald test (often not used in interpretation of lag models), it tests if the new parameters (the lag) should be included it in the model…if significant then the new variable improves the model fit and needs to be included. This is similar to the LR test and i’ve not seen a situation where one is significant and the other not. Probably why it’s not used!\n\nIn this case we have spatial autocorrelation in the residuals of the model, but the model is not an improvement on OLS — this can also be confirmed with the AIC score (we cover that later) but the lower it is the better. Here it is 293, in OLS (model 3) it was 292. The Log likelihood for model 3 (OLS) was -141, here it is -140.\n\n4.4.1.1.1 Lag impacts\nWarning according to Solymosi and Medina (2022) you must not not compare the coefficients of this to regular OLS…Why ?\nWell in OLS recall we can use the coefficients to say…a 1 unit change in the independent variable means a drop or rise in the dependent (for a 1% increase in percent with no qualifications the percent of smokers rises by 0.64 percent). BUT here the model is not consistent as the observations will change based on the weight matrix neighbours selected which might vary (almost certainly in a distance based matrix). This means we have a direct effect (standard OLS) and then an indirect effect in the model (impact of the spatial lag).\nWe can compute these direct and indirect effects using code from Solymosi and Medina (2022) and the spatialreg package. Here the impacts() function calculates the impact of the spatial lag. We can fit this to our entire spatial weights….\n\nlibrary(spdep)\n\n# weight list is just the code from the lagsarlm\nweight_list&lt;-nb2listw(ta_knn_nb, style=\"C\")\n\nimp &lt;- impacts(slag_dv_model2_queen, listw=weight_list)\n\nimp\n\nImpact measures (lag, exact):\n                                                                      Direct\npercent_no_qualification                                        0.5083288540\npercent_home_owner                                             -0.3001931060\ncensus_2018_total_personal_income_median_curp_15years_and_over -0.0003328803\n                                                                    Indirect\npercent_no_qualification                                        1.206307e-02\npercent_home_owner                                             -7.123832e-03\ncensus_2018_total_personal_income_median_curp_15years_and_over -7.899525e-06\n                                                                       Total\npercent_no_qualification                                        0.5203919197\npercent_home_owner                                             -0.3073169376\ncensus_2018_total_personal_income_median_curp_15years_and_over -0.0003407798\n\n\nNow it is appropriate to compare these coefficients to the OLS outputs…however if you have a very large matrix this might not work, instead a sparse matrix that uses approximation methods (see Solymosi and Medina (2022) and within that resource, Lesage and Pace 2009). This is beyond the scope of the content here, but essentially this makes the method faster on larger data…but only row standardised is permitted here…\n\n\n\n\n4.4.2 The spatial error model\nAnother way of coneptualising spatial dependence in regression models is not through values of the dependent variable in some areas affecting those in neighbouring areas (as they do in the spatial lag model), but in treating the spatial autocorrelation in the residuals as something that we need to deal with, perhaps reflecting some spatial autocorrelation amongst unobserved independent variables or some other mis-specification of the model.\nWard and Gleditsch (2008) characterise this model as seeing spatial autocorrelation as a nuisance rather than being particularly informative, however it can still be handled within the model, albeit slightly differently.\nThe spatial error model can be written:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\lambda w_i\\xi_i + \\epsilon_i\\]\nwhere\n\n\\(\\xi_i\\) is the spatial component of the error terms…in other words the residuals of the values in surrounding spatial units based on the weight matrix. The same rules apply as the lag (e.g. binary = sum, row standardisied = weighted average).\n\\(\\lambda\\) is a measure of correlation between neighbouring residuals.. “it indicates the extent to which the spatial component of the errors \\(\\xi_i\\) are correlated with one another for nearby observations” as per the weight matrix, Ward and Gleditsch (2008). If there is no correlation between the then this defaults to normal OLS regression\n\nFor more detail on the spatial error model, see Ward and Gleditsch (2008)\nWe can run a spatial error model on the same data below:\n\nsem_1 &lt;- errorsarlm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data, \n               nb2listw(ta_nb, style=\"C\"), \n               method = \"eigen\")\n\n\ntidy(sem_1)\n\n# A tibble: 5 × 5\n  term                                     estimate std.error statistic  p.value\n  &lt;chr&gt;                                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                               2.53e+1 3.78           6.69 2.28e-11\n2 percent_no_qualification                  5.71e-1 0.0686         8.33 0       \n3 percent_home_owner                       -2.94e-1 0.0526        -5.58 2.41e- 8\n4 census_2018_total_personal_income_media… -3.04e-4 0.0000796     -3.82 1.33e- 4\n5 lambda                                    4.32e-1 0.118          3.65 2.62e- 4\n\n\nComparing the results of the spatial error model with the spatially lagged model and the original OLS model, the suggestion here is that the spatially correlated errors in residuals lead to over/under estimations of the coefficients.\nNote, here we can compare to OLS as there is no spatial lag.\nThe \\(\\lambda\\) parameter in the spatial error model is larger than the standard error and is significant, but the \\(\\rho\\) parameter is not larger than the standard error and is not significant.\nSo we can conclude that spatial dependence might be borne in mind when interpreting the results of this regression model. This suggests to us that the smoking is not influenced by neighbouring polygons (or smokers) but the errors between the neighours are related and we may have some mis-specification of the model - e.g. a missing variable.\n\n\n4.4.3 Key advice\nThe lag model accounts for situations where the value of the dependent variable in one area might be associated with or influenced by the values of that variable in neighbouring zones (however we choose to define neighbouring in our spatial weights matrix). With our example, smoking in one neighbourhood might be related to smoking in another. You may be able to think of other examples where similar associations may occur. You might run a lag model if you identify spatial autocorrelation in the dependent variable (closer spatial units have similar values) with Moran’s I.\nThe error model deals with spatial autocorrelation (closer spatial units have similar values) of the residuals (vertical distance between your point and line of model – errors – over-predictions or under-predictions) again, potentially revealed though a Moran’s I analysis. The error model is not assuming that neighbouring independent variables are influencing the dependent variable but rather the assumption is of an issue with the specification of the model or the data used (e.g. clustered errors are due to some un-observed clustered variables not included in the model). For example, smoking prevelance may be similar in bordering neighbourhoods because residents in these neighbouring places come from similar socio-economic backgrounds and this was not included as an independent variable in the original model. There is no spatial process (no cross authority interaction) just a cluster of an un-accounted for but influential variable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#gwr",
    "href": "04_spatial_models.html#gwr",
    "title": "4  Spatial models",
    "section": "4.5 GWR",
    "text": "4.5 GWR\nRather than spatial autocorrelation causing problems with our model, it might be that a “global” regression model does not capture the full story. In some parts of our study area, the relationships between the dependent and independent variable may not exhibit the same slope coefficient.\nIf this occurs, then we have ’non-stationarity’ - this is when the global model does not represent the relationships between variables that might vary locally.\n\n4.5.1 Bandwidth\nThis part of the practical will only skirt the edges of GWR, for much more detail you should visit the GWR website which is produced and maintained by Prof Chris Brunsdon and Dr Martin Charlton who originally developed the technique.\nI should also acknowledge the guide on GWR produced by the University of Bristol, which was a great help when producing this exercise.\n\nlibrary(spgwr)\n\ncoordsW2 &lt;- st_coordinates(coordsW)\n\nta_model3_data_gwr &lt;- cbind(ta_model3_data,coordsW2)\n\ngwrbandwidth &lt;- gwr.sel(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data_gwr,  \n                        coords=cbind(ta_model3_data_gwr$X, ta_model3_data_gwr$Y),\n                  adapt=T)\n\nAdaptive q: 0.381966 CV score: 258.7724 \nAdaptive q: 0.618034 CV score: 288.1859 \nAdaptive q: 0.236068 CV score: 223.3048 \nAdaptive q: 0.145898 CV score: 210.9782 \nAdaptive q: 0.03932385 CV score: 212.4756 \nAdaptive q: 0.1017793 CV score: 202.4218 \nAdaptive q: 0.09472045 CV score: 203.8337 \nAdaptive q: 0.111242 CV score: 202.5485 \nAdaptive q: 0.1053937 CV score: 201.7625 \nAdaptive q: 0.1063105 CV score: 201.6931 \nAdaptive q: 0.1074627 CV score: 201.8976 \nAdaptive q: 0.1061616 CV score: 201.6666 \nAdaptive q: 0.1059668 CV score: 201.6645 \nAdaptive q: 0.1057479 CV score: 201.7017 \nAdaptive q: 0.1060531 CV score: 201.6499 \nAdaptive q: 0.1060938 CV score: 201.6545 \nAdaptive q: 0.1060124 CV score: 201.6567 \nAdaptive q: 0.1060531 CV score: 201.6499 \n\ngwrbandwidth\n\n[1] 0.1060531\n\n\nSetting adapt=T here means to automatically find the proportion of observations for the weighting using k nearest neighbours (an adaptive bandwidth), False would mean a global bandwidth and that would be in meters (as our data is projected).\nOccasionally data can come with longitude and latitude as columns (e.g. WGS84) and we can use this straight in the function to save making centroids, calculating the coordinates and then joining - the argument for this is longlat=TRUE and then the columns selected in the coords argument e.g. coords=cbind(long, lat). The distance result will then be in KM.\nThe optimal bandwidth is about 0.106 meaning 10.6% of all the total spatial units should be used for the local regression based on k-nearest neighbours. Which is about 7 of the 66 territorial authorities.\nThis approach uses cross validation to search for the optimal bandwidth, it compares different bandwidths to minimise model residuals — this is why we specify the regression model within gwr.sel(). It does this with a Gaussian weighting scheme (which is the default) - meaning that near points have greater influence in the regression and the influence decreases with distance - there are variations of this, but Gaussian is a fine to use in most applications. To change this we would set the argument gweight = gwr.Gauss in the gwr.sel() function — gwr.bisquare() is the other option. We don’t go into cross validation in this module.\nHowever we could set either the number of neighbours considered or the distance within which to considered points ourselves, manually, in the gwr() function below.\nTo set the number of other neighbours considered simply change the adapt argument to the value you want - it must be the number of neighbours divided by the total (e.g. to consider 20 neighbours it would be 20/66 and you’d use the value of 0.30)\nTo set the bandwidth, remove the adapt argument and replace it with bandwidth and set it, in this case, in meters.\nTo conclude, we can:\n\nset the bandwidth in gwr.sel() automatically using:\n\nthe number of neighbors\na distance threshold\n\nOr, we can set it manually in gwr() using:\n\na number of neighbors\na distance threshold\n\n\n\n\n\n\n\n\nTip\n\n\n\nBUT a problem with setting a fixed bandwidth is we assume that all variables have the same relationship across the same space (using the same number of neighbours or distance). We can let these bandwidths vary as some relationships will operate on different spatial scales…this is called Multiscale GWR and Lex Comber recently said that all GWR should be Multisacle (oops!). We have already covered a lot here so i won’t go into it. If you are interested Lex has a good tutorial on Multiscale GWR\n\n\n\n\n4.5.2 Model\n\n#run the gwr model\ngwr_model = gwr(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data_gwr,  \n                        coords=cbind(ta_model3_data_gwr$X, ta_model3_data_gwr$Y), \n                adapt=gwrbandwidth,\n                #matrix output\n                hatmatrix=TRUE,\n                #standard error\n                se.fit=TRUE)\n\n#print the results of the model\ngwr_model\n\nCall:\ngwr(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = ta_model3_data_gwr, coords = cbind(ta_model3_data_gwr$X, \n        ta_model3_data_gwr$Y), adapt = gwrbandwidth, hatmatrix = TRUE, \n    se.fit = TRUE)\nKernel function: gwr.Gauss \nAdaptive quantile: 0.1060531 (about 6 of 66 data points)\nSummary of GWR coefficient estimates at data points:\n                                                                      Min.\nX.Intercept.                                                    3.3703e+00\npercent_no_qualification                                        4.1815e-01\npercent_home_owner                                             -4.4357e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -9.2994e-04\n                                                                   1st Qu.\nX.Intercept.                                                    1.2399e+01\npercent_no_qualification                                        4.8399e-01\npercent_home_owner                                             -3.1838e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -5.2518e-04\n                                                                    Median\nX.Intercept.                                                    1.7101e+01\npercent_no_qualification                                        5.2129e-01\npercent_home_owner                                             -2.4756e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -9.6083e-05\n                                                                   3rd Qu.\nX.Intercept.                                                    3.4618e+01\npercent_no_qualification                                        5.8085e-01\npercent_home_owner                                             -1.8668e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -2.1233e-05\n                                                                      Max.\nX.Intercept.                                                    4.8631e+01\npercent_no_qualification                                        6.9995e-01\npercent_home_owner                                             -1.6115e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over  1.9089e-04\n                                                                Global\nX.Intercept.                                                   27.9622\npercent_no_qualification                                        0.5152\npercent_home_owner                                             -0.3051\ncensus_2018_total_personal_income_median_curp_15years_and_over -0.0003\nNumber of data points: 66 \nEffective number of parameters (residual: 2traceS - traceS'S): 25.09782 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 40.90218 \nSigma (residual: 2traceS - traceS'S): 1.503921 \nEffective number of parameters (model: traceS): 19.93781 \nEffective degrees of freedom (model: traceS): 46.06219 \nSigma (model: traceS): 1.417184 \nSigma (ML): 1.183931 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 272.3116 \nAIC (GWR p. 96, eq. 4.22): 229.5246 \nResidual sum of squares: 92.51173 \nQuasi-global R2: 0.9204215 \n\n\nThe output from the GWR model reveals how the coefficients vary across the 66 territorial authorities New Zealand. You will see how the global coefficients are exactly the same as the coefficients in the earlier lm model. In this particular model, if we take percent with no qualification, we can see that the coefficients range from a minimum value of 0.41 (1 unit change in percent with no qualification resulting in a rise in percent smoking of 0.41) to 0.69 (1 unit change in percent with no qualification resulting in a rise in percent smoking of 0.69).\nYou will notice that the R-Squared value (Quasi global R-squared) has improved - this is not uncommon for GWR models, but it doesn’t necessarily mean they are definitely better than global models. The small number of cases (spatial areas) under the kernel means that GW models have been criticised for lacking statistical robustness.\nThe best way to compare models is with the AIC (Akaike Information Criterion) or for smaller sample sizes the sample-size adjusted AICc, especially when you number of points is less than 40! Which it will be in GWR. The models must also be using the same data and be over the same study area!\nAIC is calculated using the:\n\nnumber of independent variables\nmaximum likelihood estimate of the model (how well the model reproduces the data).\n\nThe lower the value the better the better the model fit is, see scribbr if you want to know more here..although this is enough to get you through most situations.\nCoefficient ranges can also be seen for the other variables and they suggest some interesting spatial patterning.\n\n\n4.5.3 Map coefficients\nTo explore this we can plot the GWR coefficients for different variables. Firstly we can attach the coefficients to our original dataframe - this can be achieved simply as the coefficients for each territoiral authority appear in the same order in our spatial points dataframe as they do in the original dataframe.\n\nresults &lt;- as.data.frame(gwr_model$SDF)\nnames(results)\n\n [1] \"sum.w\"                                                                \n [2] \"X.Intercept.\"                                                         \n [3] \"percent_no_qualification\"                                             \n [4] \"percent_home_owner\"                                                   \n [5] \"census_2018_total_personal_income_median_curp_15years_and_over\"       \n [6] \"X.Intercept._se\"                                                      \n [7] \"percent_no_qualification_se\"                                          \n [8] \"percent_home_owner_se\"                                                \n [9] \"census_2018_total_personal_income_median_curp_15years_and_over_se\"    \n[10] \"gwr.e\"                                                                \n[11] \"pred\"                                                                 \n[12] \"pred.se\"                                                              \n[13] \"localR2\"                                                              \n[14] \"X.Intercept._se_EDF\"                                                  \n[15] \"percent_no_qualification_se_EDF\"                                      \n[16] \"percent_home_owner_se_EDF\"                                            \n[17] \"census_2018_total_personal_income_median_curp_15years_and_over_se_EDF\"\n[18] \"pred.se.1\"                                                            \n[19] \"coord.x\"                                                              \n[20] \"coord.y\"                                                              \n\n\n\n#attach coefficients to original\n\n\nta_model3_data_gwr_results &lt;- ta_model3_data_gwr %&gt;%\n  mutate(coefqualification = results$percent_no_qualification,\n         coefhomeowner = results$percent_home_owner,\n         coefincome = results$census_2018_total_personal_income_median_curp_15years_and_over_se)\n\n\ntm_shape(ta_model3_data_gwr_results) +\n  tm_polygons(col = \"coefqualification\", \n              palette = \"RdBu\", \n              alpha = 0.5)\n\n\n\n\n\n\n\n\nTaking the first coefficient we see that this varies across the country. A possible trend to explore further might be the inclusion or distance to tobacco retailers - both Wellington and Auckland have some of the highest coefficients, meaning a 1 unit (percent) change in no qualification returns a higher rise in percent of people smoking.\n\n\n4.5.4 Significance\nOf course, these results may not be statistically significant across the whole of New Zealand. Roughly speaking, if a coefficient estimate is more than 2 standard errors away from zero, then it is “statistically significant”.\nRemember from earlier the standard error is “the average amount the coefficient varies from the average value of the dependent variable (its standard deviation). So, for 1% increase in percent with no qualifications, while the model says we might expect GSCE scores to drop by 0.6%, this might vary, on average, by about 0.08%. As a rule of thumb, we are looking for a lower value in the standard error relative to the size of the coefficient.”\nTo calculate standard errors, for each variable you can use a formula similar to this:\n\n#run the significance test\n# abs gets the absolute vale - negative values are converted into positive value\nsigTest = abs(gwr_model$SDF$percent_no_qualification)- 2* gwr_model$SDF$percent_no_qualification_se\n\n\n#store significance results\nta_model3_data_gwr_results &lt;- ta_model3_data_gwr_results %&gt;%\n  mutate(gwrpercent_no_qualification_sig = sigTest)\n\n\ntm_shape(ta_model3_data_gwr_results) +\n  tm_polygons(col = \"gwrpercent_no_qualification_sig\", \n              palette = \"RdYlBu\")\n\n\n\n\n\n\n\n\nIf this is greater than zero (i.e. the estimate is more than two standard errors away from zero), it is very unlikely that the true value is zero, i.e. it is statistically significant (at nearly the 95% confidence level)\nThis is a combination of two ideas:\n\n95% of data in a normal distribution is within two standard deviations of the mean\nStatistical significance in a regression is normally measured at the 95% level. If the p-value is less than 5% — 0.05 — then there’s a 95% probability that a coefficient as large as you are observing didn’t occur by chance\n\nCombining these two means if…\n\nthe coefficient is large in relation to its standard error and\nthe p-value tells you if that largeness is statistically acceptable - at the 95% level (less than 5% — 0.05)\n\nYou can be confident that in your sample, nearly all of the time, that is a real and reliable coefficient value.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#including-spatial-features",
    "href": "04_spatial_models.html#including-spatial-features",
    "title": "4  Spatial models",
    "section": "4.6 Including spatial features",
    "text": "4.6 Including spatial features\nI mentioned at the start we could also try and include some spatial features within our model - for example the density of shops that sell tobacco? In a sense this is similar to what is termed a spatially omitted covariate… typically this means that the nearer something is the more influence it might have on our model (e.g. parks and house prices?).\nHowever, we could only really calculate distance to tobacco shops from the centroid of the territorial authority as a new independent variable. But instead we could use density of shops per territorial authority!\nHere, i will use open street map to calculate a density. I have downloaded the data from Geofabrik\n\nosm_pois &lt;- st_read(\"prac4_data/new-zealand-latest-free.shp/gis_osm_pois_a_free_1.shp\")\n\nReading layer `gis_osm_pois_a_free_1' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_25\\spatial analysis of public health\\spatial-analysis-of-public-health-data-25\\prac4_data\\new-zealand-latest-free.shp\\gis_osm_pois_a_free_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 59185 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -176.6404 ymin: -52.59086 xmax: 178.5443 ymax: -34.42668\nGeodetic CRS:  WGS 84\n\nosm_pois_filter &lt;- osm_pois %&gt;%\n  filter(fclass == \"supermarket\" | fclass ==\"convenience\")%&gt;%\n  st_transform(., 2135)\n\nNow let’s intersect with the territorial authority boundaries and calculate a density.\nNote that the districts object is in the correct CRS, however i beleive this is due to naming differences so we will transform it just incase.\n\nta &lt;- ta %&gt;%\n   st_transform(., 2135)\n\nstores_joined &lt;- ta %&gt;%\n  mutate(n = lengths(st_intersects(., osm_pois_filter)))%&gt;%\n  janitor::clean_names()%&gt;%\n  #calculate area\n  mutate(area=st_area(.))%&gt;%\n  #then density of the points per ward\n  mutate(density=n/area)\n\nNow, we could use this in our model!\nOn the 1st July 2024 New Zealand intends to reduce the number of tobacco retailers from 6,000 to just 599! With a maximum number per area according to the table set on Ministry of Health website.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "05_food_bank_accessibility.html",
    "href": "05_food_bank_accessibility.html",
    "title": "5  Food bank accessibility",
    "section": "",
    "text": "Note\n\n\n\nAim: Identify areas (in and around Oxford/shire) with low access and high demand for food banks\n\nConvert food bank addresses to point locations\nCreate maps/visualisations of locations of food banks (in the UK and Oxfordshire)\nCalculate accessibility for walking, driving, and public transport using r5r package\nIdentify areas with low accessibility to food banks, but a perceived high need (based on a couple of factors)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOptional reading\n\nPereira, R. et al., r5r: Rapid Realistic Routing on Multimodal Transport Networks with R5 in R (2021)\nAllen, J. and Farber, S. Changes in transit accessibility to food banks in Toronto during Covid-19 (2021)\nHiggins, C.D. et al. Changes in accessibility to emergency and community food services during Covid-19 and implications for low income populations in Hamilton, Ontario (2021)\n\n\n\n\n5.0.1 Convert food bank addresses to point locations\nFirstly, load the R packages needed for the practical. You may need to install some of these for the first time.\n\n#Load relevant packages\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(RColorBrewer)\nlibrary(ggplot2) #for graphs\nlibrary(tmap) #for maps\nlibrary(tmaptools) #for maps\nlibrary(leaflet) #for maps\nlibrary(reader)\nlibrary(readxl)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(sp)\nlibrary(sf)\nlibrary(spdep)\nlibrary(spatstat)\nlibrary(PostcodesioR) #for postcode lat and long\nlibrary(mosaic) #for favstats function\nlibrary(geodist) #for distances\nlibrary(rje) #for row mins\nlibrary(osmdata) #for osm\nlibrary(osmextract) # for osm\nlibrary(osrm) #for osm\nlibrary(r5r) #for public transport\nlibrary(tidygeocoder)\nlibrary(tidytransit) #for public transport\n\n\n\n5.0.2 Load the spatial data\nWe need to load the spatial data for different spatial scales. This polygons data will be used for creating maps and calculating accessibility to food banks centres. These were downloaded from the Open Geography Portal (from the Office for National Statistics).\n\nLower super output area (LSOA) scale\nLocal authority scale\nEntire country scale\n\nThere are 35,672 LSOAs (for England and Wales)\n\n#2021 shape file for LSOAs\nLSOA_2021 &lt;- st_read(\"LSOA_2021/LSOA_(Dec_2021)_Boundaries_Generalised_Clipped_EW_(BGC).shp\") %&gt;%\n  st_transform(., 27700) %&gt;%\n  clean_names() \n\nThere are 374 local authorities (for the UK)\n\n#2022 shape file for LAs\nLA_2022 &lt;- st_read(\"LA_2022/LAD_MAY_2022_UK_BFE_V3.shp\") %&gt;%\n  st_transform(., 27700) %&gt;%\n  clean_names() \n\n\n#2023 shape file for UK\nUK_border &lt;- st_read(\"UK_2023/CTRY_DEC_2023_UK_BUC.shp\") %&gt;%\n  st_transform(., 27700) %&gt;%\n  clean_names() \n\n\n\n5.0.3 Data on Trussell food bank centres\nNext, we load the Trussell food bank data with addresses (this data was provided by Trussell in February 2023 so some centres may have closed in the last two years or new centres may have opened).\n\n#locations of food banks from TT\nTT_foodbanks &lt;- read_excel(\"Food banks Feb23.xlsx\") %&gt;%\n  clean_names()\n\nIn order to derive point locations from postcodes, we need to clean the food bank postcodes so they are in a uniform and readable form. At this stage, we also remove food bank centres that are not currently open or are warehouses or offices (where people cannot collect food from).\n\n#removing white space from postcodes\nTT_foodbanks$postcode &lt;- gsub(\"[[:space:]]\", \"\", TT_foodbanks$postcode)\n\n#filter out only distribution centres\nTT_foodbanks &lt;- TT_foodbanks %&gt;%\n  filter(type == \"Distribution Centre\") %&gt;% #only want distribution centres and not warehouses or offices\n  filter(operating_model != \"Temporarily Closed\") #remove centres that are currently closed\n\nNext, we derive the point locations from postcodes. Unfortunately, the approach below only allows 100 locations at a time to be converted.\n\nTT_foodbanks1 &lt;- TT_foodbanks[1:100, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks1$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points1 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points1 &lt;- \n  st_as_sf(TT_foodbank_location_points1,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks2 &lt;- TT_foodbanks[101:200, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks2$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points2 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points2 &lt;- \n  st_as_sf(TT_foodbank_location_points2,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks3 &lt;- TT_foodbanks[201:300, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks3$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points3 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points3 &lt;- \n  st_as_sf(TT_foodbank_location_points3,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks4 &lt;- TT_foodbanks[301:400, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks4$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points4 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points4 &lt;- \n  st_as_sf(TT_foodbank_location_points4,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks5 &lt;- TT_foodbanks[401:500, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks5$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points5 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points5 &lt;- \n  st_as_sf(TT_foodbank_location_points5,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks6 &lt;- TT_foodbanks[501:600, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks1$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points6 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points6 &lt;- \n  st_as_sf(TT_foodbank_location_points6,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks7 &lt;- TT_foodbanks[601:700, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks7$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points7 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points7 &lt;- \n  st_as_sf(TT_foodbank_location_points7,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks8 &lt;- TT_foodbanks[701:800, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks8$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points8 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points8 &lt;- \n  st_as_sf(TT_foodbank_location_points8,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks9 &lt;- TT_foodbanks[801:900, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks9$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points9 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points9 &lt;- \n  st_as_sf(TT_foodbank_location_points9,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks10 &lt;- TT_foodbanks[901:1000, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks10$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points10 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points10 &lt;- \n  st_as_sf(TT_foodbank_location_points10,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks11 &lt;- TT_foodbanks[1001:1100, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks11$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points11 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points11 &lt;- \n  st_as_sf(TT_foodbank_location_points11,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks12 &lt;- TT_foodbanks[1101:1200, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks12$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points12 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points12 &lt;- \n  st_as_sf(TT_foodbank_location_points12,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks13 &lt;- TT_foodbanks[1201:1300, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks13$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points13 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points13 &lt;- \n  st_as_sf(TT_foodbank_location_points13,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks14 &lt;- TT_foodbanks[1301:1400, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks14$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points14 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points14 &lt;- \n  st_as_sf(TT_foodbank_location_points14,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks15 &lt;- TT_foodbanks[1401:1425, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks15$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points15 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points15 &lt;- \n  st_as_sf(TT_foodbank_location_points15,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nAfter all the Trussell centres have had their point location derived, we bind them together in one data frame.\n\nTT_foodbank_location_points &lt;- rbind(TT_foodbank_location_points1, TT_foodbank_location_points2,\n                                     TT_foodbank_location_points3, TT_foodbank_location_points4, \n                                     TT_foodbank_location_points5, TT_foodbank_location_points6,\n                                     TT_foodbank_location_points7, TT_foodbank_location_points8,\n                                     TT_foodbank_location_points9, TT_foodbank_location_points10,\n                                     TT_foodbank_location_points11, TT_foodbank_location_points12,\n                                     TT_foodbank_location_points13, TT_foodbank_location_points14,\n                                     TT_foodbank_location_points15)\n\n\nnrow(TT_foodbanks) - nrow(TT_foodbank_location_points)\n\nThis process has lost 42 food bank centres as postcode could not be recognised/matched with a longitude and latitude.\nWe need to join the point location to the other information on the food bank centres like the centre’s name and the operating model etc.\n\n#removing white space from postcodes in order to merge with more information\nTT_foodbank_location_points$postcode &lt;- gsub(\"[[:space:]]\", \"\", TT_foodbank_location_points$postcode)\n\n#joining point data with other information on food bank centres e.g. name and operating model\nTT_foodbank_location_points &lt;- TT_foodbank_location_points %&gt;%\n  merge(.,\n        TT_foodbanks, \n        by.x = \"postcode\",\n        by.y = \"postcode\",\n        all.x = TRUE)\n\n#select specific columns\nTT_foodbank_location_points &lt;- TT_foodbank_location_points %&gt;%\n  distinct(foodbank_centre_number, postcode, .keep_all = TRUE) %&gt;%\n  dplyr::select(food_bank_name, distribution_centre_name, foodbank_centre_number, postcode, operating_model, number_of_distribution_centres_final)\n\n\n\n5.0.4 Create a map of Trussell food bank locations in the UK\nNow we have the point location of Trussell centres, we can use the tmap package to visualise where the Trussell food bank centres are across the UK by making a simple map.\n\nTT_foodbank_location_points &lt;- TT_foodbank_location_points %&gt;%\n  st_transform(., 27700) \n\n#clips the point locations to be only within the shape file area - excludes food banks outside the boundary\nTT_foodbank_location_points &lt;- TT_foodbank_location_points[UK_border,] \n\n#plot map\nPlain_map &lt;- \n  tm_shape(UK_border) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(TT_foodbank_location_points) +\n  tm_symbols(col = \"forestgreen\", size = 0.03) +\n  tm_scale_bar(position = c(\"left\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(main.title = \"Trussell food bank centres\", \n            main.title.position = \"centre\",\n            main.title.size = 0.7, \n            legend.outside = TRUE, frame = FALSE)\n\nPlain_map\n\n\n\n\n\n\n\nNote\n\n\n\nQuestions\n\nWhat does this tell up about the spatial distribution of food banks?\nWhat could be added to this map to enhance the visualisation?\n\n\n\n\n\n5.0.5 Create a map of Trussell food bank locations in Oxford and Oxfordshire\nLet’s zoom in on Oxford and the surrounding area.\n\n#use string detect function to find local authorities with these words in their names\nOxford_LAs &lt;- LA_2022 %&gt;%\n  filter(str_detect(lad22nm, \"Oxford\") | \n           str_detect(lad22nm, \"White Horse\") | \n           str_detect(lad22nm, \"Cherwell\")) \n\n\n#use string detect function to isolate the border of the city of Oxford\nOxford_City_LAs &lt;- LA_2022 %&gt;%\n  filter(str_detect(lad22cd, \"E07000178\")) \n\n\n#find food bank centres from within the border we have created\nOxford_TT_foodbanks &lt;- TT_foodbank_location_points[Oxford_LAs,] \n\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(Oxford_TT_foodbanks) +\n  tm_symbols(col = \"deepskyblue\", size = 0.1) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(main.title = \"Trussell centres in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9, \n            legend.outside = TRUE, frame = FALSE)\n\nOxford_map\n\n\ntmap_mode(\"view\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(Oxford_TT_foodbanks) +\n  tm_symbols(col = \"deepskyblue\", size = 0.1) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n    tm_layout(main.title = \"Trussell centres in/around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9, \n            legend.outside = TRUE, frame = FALSE)\n\nOxford_map\n\nWe see only seven food bank centres across the region, and no food bank centres in the City of Oxford itself, so we look at other sources of data to see if this is true.\n\n5.0.5.1 Give Food food bank data\nGive Food is an open access database with information on food banks in the UK. Next, load the food bank data with addresses (from Give Food). However, these are food banks and not distribution centres.\n\n#data from give food\ngive_food_foodbanks &lt;- read_csv(\"foodbanks.csv\") %&gt;%\n  clean_names()\n\n\n#removing white space from postcodes\ngive_food_foodbanks$postcode &lt;- gsub(\"[[:space:]]\", \"\", give_food_foodbanks$postcode)\n\n#filter out only distribution centres\ngive_food_foodbanks &lt;- give_food_foodbanks %&gt;%\n  filter(closed != \"TRUE\") #remove centres that are currently closed\n\n\n#create 2 versions\nOxford_give_food_foodbanks &lt;- give_food_foodbanks %&gt;%\n  filter(str_detect(postcode, \"OX\")) \n\nOxford_give_food_foodbanks2 &lt;- give_food_foodbanks %&gt;%\n  filter(str_detect(postcode, \"OX\")) \n\n\nOxford_give_food_foodbanks2$postcode &lt;- gsub(\"[[:space:]]\", \"\", Oxford_give_food_foodbanks2$postcode)\n\n#create spatial object\nOxford_give_food_foodbanks2 &lt;- Oxford_give_food_foodbanks2 %&gt;%\n  st_transform(., 27700) \n\n#select only specific variables needed\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks %&gt;%\n  select(name, postcode, district, network)\n\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks2 %&gt;%\n  merge(.,\n        Oxford_give_food_foodbanks, \n        by.x = \"postcode\",\n        by.y = \"postcode\",\n        all.x = TRUE)\n\n\n#create spatial object\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks %&gt;%\n  st_transform(., 27700) \n\ntmap_mode(\"plot\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(Oxford_give_food_foodbanks) +\n  tm_symbols(col = \"red\", size = 0.1) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(main.title = \"Other food banks in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9, \n            legend.outside = TRUE, frame = FALSE)\n\nOxford_map\n\n\ntmap_mode(\"view\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(Oxford_give_food_foodbanks) +\n  tm_symbols(col = \"red\", size = 0.1) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n    tm_layout(main.title = \"Other food banks in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9, \n            legend.outside = TRUE, frame = FALSE)\n\nOxford_map\n\nIf you hover over the food bank centres on the interactive map, information on the centre that is contained in the data frame will pop up in a box.\nNext we map the same data, but spilt the centres by network type.\n\ntrussell_Oxford &lt;- Oxford_give_food_foodbanks %&gt;%\n  filter(network == \"Trussell\")\n\nIFAN_Oxford &lt;- Oxford_give_food_foodbanks %&gt;%\n  filter(network == \"IFAN\")\n\nindependent_Oxford &lt;- Oxford_give_food_foodbanks %&gt;%\n  filter(network == \"Independent\")\n\n#change mode from interactive to static\ntmap_mode(\"plot\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(trussell_Oxford) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(IFAN_Oxford) +\n    tm_symbols(col = \"orange\", size = 0.15) +\n  tm_shape(independent_Oxford) +\n    tm_symbols(col = \"red\", size = 0.15) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Other food banks in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) +\n  tm_add_legend('symbol', \n    col = c(\"purple\", \"orange\", \"red\"),\n    border.col = \"black\",\n    size = 1,\n    labels = c('Trussell',\n               'IFAN',\n               'Independent'))\n\nOxford_map\n\nExamining this data informs us that there are more food banks in the area than simply Trussell centres. However, before\n\n#change mode from interactive to static\ntmap_mode(\"plot\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_TT_foodbanks) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(trussell_Oxford) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(IFAN_Oxford) +\n    tm_symbols(col = \"orange\", size = 0.15) +\n  tm_shape(independent_Oxford) +\n    tm_symbols(col = \"red\", size = 0.15) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Other food banks in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) +\n  tm_add_legend('symbol', \n    col = c(\"purple\", \"orange\", \"red\"),\n    border.col = \"black\",\n    size = 1,\n    labels = c('Trussell',\n               'IFAN',\n               'Independent'))\n\nOxford_map\n\n\n\n\n5.0.6 Calculate accessibility for walking, driving, and public transport using r5r package\nr5r is an R package for rapid realistic routing on multi-modal transport networks (walk, bike, public transport and car).\nTo use r5r, we will need:\n\nA road network data set from OpenStreetMap in .pbf format (mandatory)\nA public transport feed in GTFS.zip format (optional)\nA raster file of Digital Elevation Model data in .tif format (optional)\n\nLink to R package page: https://ipeagit.github.io/r5r/\n\n5.0.6.1 Centroids from LSOAs\nFirst we need to get the point locations for LSOA centroids for Oxford and the surrounding area. You can also download the population weighted centroids for LSOAs, but we are not using those today.\nRepeat the filtering process for LSOAs like we did for LAs.\n\nOxford_LSOAs &lt;- LSOA_2021 %&gt;%\n  filter(str_detect(lsoa21nm, \"Oxford\") | \n           str_detect(lsoa21nm, \"White Horse\") | \n           str_detect(lsoa21nm, \"Cherwell\")) \n\n\n#change mode from interactive to static\ntmap_mode(\"plot\")\n\n#plot map\nOxford_map_LSOAs &lt;- \n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"white\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_TT_foodbanks) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(trussell_Oxford) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(IFAN_Oxford) +\n    tm_symbols(col = \"orange\", size = 0.15) +\n  tm_shape(independent_Oxford) +\n    tm_symbols(col = \"red\", size = 0.15) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Food bank centres in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) +\n  tm_add_legend('symbol', \n    col = c(\"purple\", \"orange\", \"red\"),\n    border.col = \"black\",\n    size = 1,\n    labels = c('Trussell',\n               'IFAN',\n               'Independent'))\n\nOxford_map_LSOAs\n\nBy mapping food bank centres are the LSOA scale rather than the LA scale, we can assume that they are usually located in more densely populated areas like towns/villages in the area.\n\nsf_use_s2(FALSE)\n\ncentroids &lt;- st_centroid(Oxford_LSOAs) %&gt;%\n  st_transform(.,4326)\n  \ncentroids &lt;- centroids %&gt;%\n  mutate(long = unlist(map(centroids$geometry,1)),\n           lat = unlist(map(centroids$geometry,2)))\n\ncentroids$key &lt;- 1\ncentroids$geometry &lt;- NULL\n\ncentroids &lt;- centroids %&gt;%\n  dplyr::select(lsoa21cd, long, lat, key)\n\n\n# load in origin locations - LSOA centroids\npoints_o &lt;- centroids\npoints_o$lon &lt;- as.numeric(points_o$long)\npoints_o$lat &lt;- as.numeric(points_o$lat)\npoints_o$id &lt;- points_o$lsoa21cd\npoints_o &lt;- points_o[,c(\"id\",\"lon\",\"lat\")]\n\n#look at the data\nhead(points_o,10)\n\n\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks %&gt;%\n  sf::st_transform(crs = 4326)\n\n#unlist geometry column\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks %&gt;%\n  mutate(longitude = unlist(map(Oxford_give_food_foodbanks$geometry,1)),\n           latitude = unlist(map(Oxford_give_food_foodbanks$geometry,2)))\n\npoints_d &lt;- Oxford_give_food_foodbanks\npoints_d$lon &lt;- as.numeric(points_d$longitude)\npoints_d$lat &lt;- as.numeric(points_d$latitude)\npoints_d$id &lt;- points_d$postcode\npoints_d &lt;- points_d[,c(\"id\",\"lon\",\"lat\")]\n\n\n#look at the data\nhead(points_d,10)\n\nThe next code chunk downloads the road network for the area. However, for this practical, it is included in the data provided (as is the timetable data for public transport).\n\n# load libraries\n#library(r5r)\n#https://download.geofabrik.de/europe.html\n\n#osm_lines = oe_get(\"Oxfordshire\", stringsAsFactors = FALSE, quiet = TRUE)\n#plot(st_geometry(osm_lines))\n\n#EW_url = oe_match(\"Oxfordshire\")\n#oe_download(\n  #file_url = EW_url$url,\n  #file_size = EW_url$file_size,\n  #download_directory = tempdir())\n\n#EW_url[[\"url\"]]\n\nNext we need to set up the r5r core. This sets up the road network and\n\n# increase Java memory\noptions(java.parameters = \"-Xmx6G\")\n\n# load libraries\nlibrary(r5r)\n\n#path &lt;- system.file(\"r5r_path\", package = \"r5r\")\n\nr5r_core &lt;- setup_r5(data_path = \".\")\n\nFor this example, I set the maximum trip duration to 1 hour (60 minutes) and set the departure time at 2pm.\n\n# routing paramaters\nmode &lt;- c(\"TRANSIT\")\n\nmax_trip_duration &lt;- 60 # minutes\n\ndeparture_datetime &lt;- as.POSIXct(\"01-03-2021 14:00:00\", \n                                 format = \"%d-%m-%Y %H:%M:%S\",\n                                 tz = \"Europe/London\")\n\n# estimating the travel time\nTTM_Oxford_PT &lt;- travel_time_matrix(r5r_core,\n                          origins = points_o,\n                          destinations = points_d,\n                          mode = mode,\n                          departure_datetime = departure_datetime,\n                          max_trip_duration = max_trip_duration)\n\n\n# routing paramaters\nmode &lt;- c(\"WALK\")\n\nmax_trip_duration &lt;- 60  # minutes\n\ndeparture_datetime &lt;- as.POSIXct(\"01-03-2021 14:00:00\", \n                                 format = \"%d-%m-%Y %H:%M:%S\",\n                                 tz = \"Europe/London\")\n\n# estimating the travel time\nTTM_Oxford_walk &lt;- travel_time_matrix(r5r_core,\n                          origins = points_o,\n                          destinations = points_d,\n                          mode = mode,\n                          departure_datetime = departure_datetime,\n                          max_trip_duration = max_trip_duration)\n\n\n# routing paramaters\nmode &lt;- c(\"CAR\")\n\nmax_trip_duration &lt;- 60  # minutes\n\ndeparture_datetime &lt;- as.POSIXct(\"01-03-2021 14:00:00\", \n                                 format = \"%d-%m-%Y %H:%M:%S\",\n                                 tz = \"Europe/London\")\n\n# estimating the travel time\nTTM_Oxford_drive &lt;- travel_time_matrix(r5r_core,\n                          origins = points_o,\n                          destinations = points_d,\n                          mode = mode,\n                          departure_datetime = departure_datetime,\n                          max_trip_duration = max_trip_duration)\n\nWe now have three modes of transport (measure of access) to food banks in the area - public transport, walking, and driving. As you can see many more LSOAs have access to a food bank centre within a one hour drive, compared to one hour walking or public transport.\n\n\n\n5.0.7 Getting minimum travel time\nLet’s start with walking.\n\n#select minimum value between pairs \nTTM_Oxford_walk &lt;- TTM_Oxford_walk %&gt;%\n  group_by(to_id, from_id) %&gt;% \n  slice(which.min(travel_time_p50))\n\n#check travel times as numeric\nTTM_Oxford_walk$travel_time_p50 &lt;- as.numeric(TTM_Oxford_walk$travel_time_p50)\n\nThis removes origin-destination pairs that have two or more values for travel time under one hour\n\n#pivot wider\nTTM_Oxford_walk &lt;- TTM_Oxford_walk %&gt;%\n  pivot_wider(names_from = to_id, values_from = travel_time_p50)\n\nhead(TTM_Oxford_walk)\n\nThe number of rows has decreased as not all LSOAs will have accessibility to a centre within one hour. Let’s repeat this for driving and public transport.\n\n#select minimum value between pairs \nTTM_Oxford_drive &lt;- TTM_Oxford_drive %&gt;%\n  group_by(to_id, from_id) %&gt;% \n  slice(which.min(travel_time_p50))\n\n#check travel times as numeric\nTTM_Oxford_drive$travel_time_p50 &lt;- as.numeric(TTM_Oxford_drive$travel_time_p50)\n\n#pivot wider to create a matrix\nTTM_Oxford_drive &lt;- TTM_Oxford_drive %&gt;%\n  pivot_wider(names_from = to_id, values_from = travel_time_p50)\n\nhead(TTM_Oxford_drive)\n\n\n#select minimum value between pairs \nTTM_Oxford_PT &lt;- TTM_Oxford_PT %&gt;%\n  group_by(to_id, from_id) %&gt;% \n  slice(which.min(travel_time_p50))\n\n#check travel times as numeric\nTTM_Oxford_PT$travel_time_p50 &lt;- as.numeric(TTM_Oxford_PT$travel_time_p50)\n\n#pivot wider to create a matrix\nTTM_Oxford_PT &lt;- TTM_Oxford_PT %&gt;%\n  pivot_wider(names_from = to_id, values_from = travel_time_p50)\n\nhead(TTM_Oxford_PT)\n\nNow we have the accessibility data in a usable form that we can join to our Oxford LSOAs data frame, we can calculate mean and minimum travel times for each LSOA for each mode of transport.\n\n#mean time\nTTM_Oxford_drive$mean_time_drive &lt;- rowMeans(TTM_Oxford_drive[,c(2:15)], na.rm = TRUE)\n\nTTM_Oxford_PT$mean_time_PT &lt;- rowMeans(TTM_Oxford_PT[,c(2:15)], na.rm = TRUE)\n\nTTM_Oxford_walk$mean_time_walk &lt;- rowMeans(TTM_Oxford_walk[,c(2:15)], na.rm = TRUE)\n\n#minimum time\nTTM_Oxford_drive$minimum_time_drive &lt;- apply(TTM_Oxford_drive, 1, FUN=min, na.rm = TRUE)\n\nTTM_Oxford_PT$minimum_time_PT &lt;- apply(TTM_Oxford_PT, 1, FUN=min, na.rm = TRUE)\n\nTTM_Oxford_walk$minimum_time_walk &lt;- apply(TTM_Oxford_walk, 1, FUN=min, na.rm = TRUE)\n\n\n#change minimum times to numeric\nTTM_Oxford_drive$minimum_time_drive &lt;- as.numeric(TTM_Oxford_drive$minimum_time_drive)\n\nTTM_Oxford_PT$minimum_time_PT &lt;- as.numeric(TTM_Oxford_PT$minimum_time_PT)\n\nTTM_Oxford_walk$minimum_time_walk &lt;- as.numeric(TTM_Oxford_walk$minimum_time_walk)\n\nNow we need to join this to the Oxford LSOAs dataframe and map it.\n\n#rename to from id columns for joining purposes\n#select only the ID column and mean and minimum times\n\nTTM_Oxford_drive &lt;- TTM_Oxford_drive %&gt;%\n  select(from_id, mean_time_drive, minimum_time_drive) %&gt;%\n  rename(., lsoa21cd = \"from_id\")\n\nTTM_Oxford_walk &lt;- TTM_Oxford_walk %&gt;%\n  select(from_id, mean_time_walk, minimum_time_walk) %&gt;%\n  rename(., lsoa21cd = \"from_id\")\n\nTTM_Oxford_PT &lt;- TTM_Oxford_PT %&gt;%\n  select(from_id, mean_time_PT, minimum_time_PT) %&gt;%\n  rename(., lsoa21cd = \"from_id\")\n\nOxford_LSOAs &lt;- Oxford_LSOAs %&gt;%\n  left_join(.,\n            TTM_Oxford_drive,\n            by = \"lsoa21cd\") %&gt;%\n  left_join(.,\n            TTM_Oxford_walk,\n            by = \"lsoa21cd\") %&gt;%\n  left_join(.,\n            TTM_Oxford_PT,\n            by = \"lsoa21cd\")\n\n\n#map for minimum public transport time\nTravel_time_map &lt;-\n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"minimum_time_PT\", title = \"Minimum time to food bank (mins)\", style = \"cont\", n=10, palette = \"-Spectral\",\n          colorNA = \"white\", # colour of missing data\n          textNA = \"Public transport beyond one hour\") +\n    tm_borders(col = \"grey\", lwd = 0.01) +\n  tm_shape(Oxford_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_City_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"PT accessibility in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) \n\nTravel_time_map \n\n\n#map for minimum driving time\nTravel_time_map &lt;-\n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"minimum_time_drive\", title = \"Minimum time to food bank (mins)\", style = \"cont\", n=10, palette = \"-Spectral\",\n          colorNA = \"white\", # colour of missing data\n          textNA = \"Driving beyond one hour\") +\n    tm_borders(col = \"grey\", lwd = 0.01) +\n  tm_shape(Oxford_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_City_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Driving accessibility in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) \n\nTravel_time_map \n\n\n#map for minimum walking time\nTravel_time_map &lt;-\n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"minimum_time_walk\", title = \"Minimum time to food bank (mins)\", style = \"cont\", n=10, palette = \"-Spectral\",\n          colorNA = \"white\", # colour of missing data\n          textNA = \"Walking beyond one hour\") +\n    tm_borders(col = \"grey\", lwd = 0.01) +\n  tm_shape(Oxford_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_City_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Walking accessibility in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) \n\nTravel_time_map \n\nFrom these three maps it is very clear that areas have unequal access to food bank centres, especially in the rural parts of Oxfordshire. However, do people in these areas of low accessibility need greater access to centres.\n\n\n5.0.8 Examining demand\nIn this last section we will examine two determinants of food bank use. Let’s take housing tenure (socially rented homes) and disability. This is census data from 2021 at the LSOA scale.\n\n#Type of tenure of housing\nTenure &lt;- read_csv(\"Census 2021 data/Tenure.csv\") %&gt;%\n  clean_names() %&gt;%\n  separate(lsoa21cd, c('lsoa_code', 'lsoa_name'), sep = ':') %&gt;%\n  mutate(lsoa_name = trimws(lsoa_name)) %&gt;%\n  mutate(lsoa_code = trimws(lsoa_code)) \n\n#select socially rented variable\nTenure &lt;- Tenure %&gt;%\n  select(lsoa_code, lsoa_name, social_rented)\n\n#households that are socially rented\nTenure$social_rented &lt;- as.numeric(Tenure$social_rented)\nfavstats(Tenure$social_rented)\n\n#social rented\nggplot(Tenure,\n       aes(x=social_rented)) + \n  geom_histogram(aes(y=after_stat(density)), color=\"black\", fill=\"white\") +\n  geom_vline(aes(xintercept=mean(social_rented)),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n geom_density(alpha=.2, fill=\"#FF6666\")\n\n\n#Disability under the Equality Act\nDisability &lt;- read_csv(\"Census 2021 data/Disability.csv\") %&gt;%\n  clean_names() %&gt;%\n  separate(lsoa, c('lsoa_code', 'lsoa_name'), sep = ':') %&gt;%\n  mutate(lsoa_name = trimws(lsoa_name)) %&gt;%\n  mutate(lsoa_code = trimws(lsoa_code)) \n\n#select socially rented variable\nDisability &lt;- Disability %&gt;%\n  select(lsoa_code, lsoa_name, disabled_under_the_equality_act)\n\n#households with 1+ disabled person\nDisability$disabled_under_the_equality_act &lt;- as.numeric(Disability$disabled_under_the_equality_act)\nfavstats(Disability$disabled_under_the_equality_act)\n\n#disability\nggplot(Disability,\n       aes(x=disabled_under_the_equality_act)) + \n  geom_histogram(aes(y=after_stat(density)), color=\"black\", fill=\"white\") +\n  geom_vline(aes(xintercept=mean(disabled_under_the_equality_act)),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n geom_density(alpha=.2, fill=\"#FF6666\") \n\n\n\n\n\n\n\nNote\n\n\n\nQuestion:\nWhat can we say about the distribution of these two demand variables? And how could we customise these histograms to improve the look of them?\n\n\n\n#rename lsoa code variable\nDisability &lt;- Disability %&gt;%\n  rename(., \"lsoa21cd\" = \"lsoa_code\") %&gt;%\n  rename(., \"lsoa21nm\" = \"lsoa_name\")\n\nTenure &lt;- Tenure %&gt;%\n  rename(., \"lsoa21cd\" = \"lsoa_code\") %&gt;%\n  rename(., \"lsoa21nm\" = \"lsoa_name\")\n\n#join to Oxford LSOA data frame\nOxford_LSOAs &lt;- Oxford_LSOAs %&gt;%\n  left_join(.,\n            Disability, \n            by = \"lsoa21cd\") %&gt;%\n  left_join(.,\n            Tenure, \n            by = \"lsoa21cd\")\n\nLet’s make a map of disability (% of households with one or more disabled person living there) and socially rented homes.\n\n\n5.0.9 Identifying under-served areas\nFirstly, what are the national averages for disability and social renting across England and Wales?\n\nmean(Tenure$social_rented, na.rm = TRUE)\n\nmean(Disability$disabled_under_the_equality_act, na.rm = TRUE)\n\n\nSocially renting - 16.9% of households\nDisability - 17.7% of households\n\nAnd what are the Oxford and surrounding area averages?\n\nmean(Oxford_LSOAs$social_rented, na.rm = TRUE)\n\nmean(Oxford_LSOAs$disabled_under_the_equality_act, na.rm = TRUE)\n\n\nSocially renting - 14.5% of households\nDisability - 14.6% of households\n\nThey are lower than the national averages.\nLet’s take a look for the three travel times too.\n\nmean(Oxford_LSOAs$mean_time_drive, na.rm = TRUE)\n\nmean(Oxford_LSOAs$mean_time_walk, na.rm = TRUE)\n\nmean(Oxford_LSOAs$mean_time_PT, na.rm = TRUE)\n\nVery simply, if we take the areas that are below the area average for social renting and disability and one of the accessibility measures, which places in and around Oxford are under served to food banks based on high demand and lower accessibility?\n\n#set some parameters to identify which areas are under served\nOxford_LSOAs &lt;- Oxford_LSOAs %&gt;%\n  mutate(Underserved_areas = case_when\n         (social_rented &gt;= 14.5 & disabled_under_the_equality_act &gt;= 14.6 & mean_time_walk &gt;= 32 ~ \"Underserved\", \n        TRUE ~ \"Served\"))\n\n\n#creating a buffer around the LSOAs\nbuffer &lt;- Oxford_LSOAs %&gt;%\n  filter(Underserved_areas == \"Underserved\") %&gt;%\n  st_buffer(., 500) %&gt;%\n  st_union()\n\n Underserved_map &lt;-\n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"Underserved_areas\", style = \"cont\", title = \"Underserved Areas\", palette = \"Blues\") +\n  tm_borders(col = \"grey\", lwd = 0.1) +\n  tm_shape(buffer) +\n  tm_polygons(alpha = 0.1) +\n  tm_shape(Oxford_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_City_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(main.title = \"Under-served areas to food banks\", main.title.position = \"centre\",\n            legend.outside = TRUE, frame = FALSE, main.title.size = 1) \n\nUnderserved_map\n\n\n\n5.0.10 Summary\nWe can use accessibility tools and census data to assess where in an area is under-served to particular services.\n\nHow would we improve this analysis?\nWhat are the limitations of the data used?\nHow could we improve the visualisations?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Food bank accessibility</span>"
    ]
  },
  {
    "objectID": "06_datasets.html",
    "href": "06_datasets.html",
    "title": "6  Datasets",
    "section": "",
    "text": "Health data can be hard to find online. Here are some interesting datasets i have come across:\n\nUK Small area mental health index / other mental health data\nUK Understanding society health and wellbeing\nUK Police data (e.g. for looking at violence and mental wellbeing)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Datasets</span>"
    ]
  }
]